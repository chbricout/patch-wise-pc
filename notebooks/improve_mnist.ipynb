{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63aed413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cirkit\n",
    "import numpy as np\n",
    "from cirkit.templates import data_modalities, utils\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "PIXEL_RANGE = 255\n",
    "example_image = None\n",
    "\n",
    "KERNEL_SIZE = (4, 4)\n",
    "CIFAR_SIZE = (28,28)\n",
    "DEVICE = \"cuda:1\"\n",
    "EPOCH = 30\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9fdfac",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb33551",
   "metadata": {},
   "source": [
    "Let's define a function to create and use patches of the base Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ab5373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(kernel_size, stride, compile=True, contiguous_output=False):\n",
    "    kh, kw = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
    "    sh, sw = (stride, stride) if isinstance(stride, int) else stride\n",
    "\n",
    "    def _patchify(image: torch.Tensor):\n",
    "        # Accept (C,H,W) or (B,C,H,W)\n",
    "\n",
    "        # Ensure contiguous NCHW for predictable strides\n",
    "        x = image.contiguous()  # (B,C,H,W)\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Number of patches along H/W\n",
    "        Lh = (H - kh) // sh + 1\n",
    "        Lw = (W - kw) // sw + 1\n",
    "\n",
    "        # Create a zero-copy view: (B, C, Lh, Lw, kh, kw)\n",
    "        sN, sC, sH, sW = x.stride()\n",
    "        patches = x.as_strided(\n",
    "            size=(B, C, Lh, Lw, kh, kw),\n",
    "            stride=(sN, sC, sH * sh, sW * sw, sH, sW),\n",
    "        )\n",
    "        # Reorder to (B, P, C, kh, kw) where P = Lh*Lw\n",
    "        patches = patches.permute(0, 2, 3, 1, 4, 5).reshape(B * Lh * Lw, C, kh, kw)\n",
    "\n",
    "        if contiguous_output:\n",
    "            patches = (\n",
    "                patches.contiguous()\n",
    "            )  # materialize if the next ops need contiguous\n",
    "\n",
    "        return patches\n",
    "\n",
    "    if compile:\n",
    "        _patchify = torch.compile(_patchify, fullgraph=True, dynamic=False)\n",
    "    return _patchify\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: (PIXEL_RANGE * x).long()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_train = datasets.MNIST(\"datasets\", train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST(\"datasets\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Instantiate the training and testing data loaders\n",
    "train_dataloader = DataLoader(data_train, shuffle=True, drop_last=True, batch_size=256)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa33a031",
   "metadata": {},
   "source": [
    "## Defining the Circuit\n",
    "\n",
    "We want to create a factory to create the different circuit we will want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b2a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_share_circuit_factory(\n",
    "    kernel_size, region_graph, layer_type, num_units, big_region_graph=None\n",
    "):\n",
    "    shared_circ = data_modalities.image_data(\n",
    "        (1, *kernel_size),\n",
    "        region_graph=region_graph,\n",
    "        input_layer=\"categorical\",\n",
    "        num_input_units=num_units,\n",
    "        sum_product_layer=layer_type,\n",
    "        num_sum_units=num_units,\n",
    "        num_classes=num_units,\n",
    "        sum_weight_param=utils.Parameterization(\n",
    "            activation=\"softmax\", initialization=\"normal\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    big_circ = data_modalities.image_data(\n",
    "        (1, *CIFAR_SIZE),\n",
    "        region_graph=big_region_graph if big_region_graph is not None else region_graph,\n",
    "        input_layer=\"categorical\",\n",
    "        num_input_units=num_units,\n",
    "        sum_product_layer=layer_type,\n",
    "        num_sum_units=num_units,\n",
    "        sum_weight_param=utils.Parameterization(\n",
    "            activation=\"softmax\", initialization=\"normal\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return share_scope(big_circ, shared_circ, math.prod(kernel_size))\n",
    "\n",
    "def patch_circuit_factory(\n",
    "    kernel_size, region_graph, layer_type, num_units, big_region_graph=None\n",
    "):\n",
    "    shared_circ = data_modalities.image_data(\n",
    "        (1, *kernel_size),\n",
    "        region_graph=region_graph,\n",
    "        input_layer=\"categorical\",\n",
    "        num_input_units=num_units,\n",
    "        sum_product_layer=layer_type,\n",
    "        num_sum_units=num_units,\n",
    "        num_classes=num_units,\n",
    "        sum_weight_param=utils.Parameterization(\n",
    "            activation=\"softmax\", initialization=\"normal\"\n",
    "        ),\n",
    "    )\n",
    "    return shared_circ\n",
    "\n",
    "def top_circuit_factory(\n",
    "    subspace_size, region_graph, layer_type, num_units, big_region_graph=None\n",
    "):\n",
    "    shared_circ = data_modalities.image_data(\n",
    "        (1, *subspace_size),\n",
    "        region_graph=region_graph,\n",
    "        input_layer=\"categorical\",\n",
    "        num_input_units=num_units,\n",
    "        sum_product_layer=layer_type,\n",
    "        num_sum_units=num_units,\n",
    "        num_classes=1,\n",
    "        sum_weight_param=utils.Parameterization(\n",
    "            activation=\"softmax\", initialization=\"normal\"\n",
    "        ),\n",
    "    )\n",
    "    return shared_circ\n",
    "\n",
    "def base_circuit_factory(region_graph, layer_type, num_units):\n",
    "    return data_modalities.image_data(\n",
    "        (1, *CIFAR_SIZE),\n",
    "        # (1,*kernel_size),\n",
    "        region_graph=region_graph,\n",
    "        input_layer=\"categorical\",\n",
    "        num_input_units=num_units,\n",
    "        sum_product_layer=layer_type,\n",
    "        num_sum_units=num_units,\n",
    "        sum_weight_param=utils.Parameterization(\n",
    "            activation=\"softmax\", initialization=\"normal\"\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5589b276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51282690\n"
     ]
    }
   ],
   "source": [
    "from cirkit.pipeline import compile as cirkit_compile\n",
    "big_circ = cirkit_compile(base_circuit_factory(\"quad-graph\", \"cp-t\", 128))\n",
    "patch_circ = cirkit_compile(patch_circuit_factory((2,2),\"quad-graph\", \"cp-t\", 128))\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in big_circ.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9089462f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from cirkit.backend.torch.layers.input import TorchCategoricalLayer\n",
    "from cirkit.backend.torch.parameters.parameter import TorchParameter, TorchParameterNode, FoldIndexInfo\n",
    "from cirkit.backend.torch.parameters.nodes import TorchParameterInput\n",
    "from cirkit.backend.torch.graph.folding import build_unfold_index_info\n",
    "from torch import Tensor\n",
    "from cirkit.backend.torch.semiring import Semiring\n",
    "from typing import Sequence, Mapping\n",
    "import functools\n",
    "\n",
    "\n",
    "def share_parameter(graph:TorchParameter, new_fold:int):\n",
    "    new_param_nodes = []\n",
    "    for i,n in enumerate(graph.topological_ordering()):\n",
    "        instance = type(n)\n",
    "        new_conf=n.config\n",
    "        new_conf[\"num_folds\"]=new_fold\n",
    "        if \"initializer_\" in new_conf:\n",
    "            reduced_initializer_list = new_conf[\"initializer_\"].keywords[\"initializers\"][:new_fold]\n",
    "            new_conf[\"initializer_\"]= functools.partial(new_conf[\"initializer_\"].func, initializers=reduced_initializer_list)\n",
    "        if \"shape\" in new_conf:\n",
    "            shape=new_conf[\"shape\"]\n",
    "            del new_conf[\"shape\"]\n",
    "            new_param = instance(*shape, **new_conf)\n",
    "        else:\n",
    "            new_param = instance( **new_conf)\n",
    "        new_param.reset_parameters()\n",
    "        new_param_nodes.append(new_param)\n",
    "   \n",
    "    shared= TorchSharedParameter(graph.shape, new_param_nodes, num_folds=graph.num_folds)\n",
    "    fold_idx_info = FoldIndexInfo(\n",
    "        ordering=[shared],\n",
    "        in_fold_idx={0:[[]]},\n",
    "        out_fold_idx=[(0, f) for f in range(graph.num_folds)]\n",
    "    )\n",
    "    return TorchParameter([shared], {shared:[]}, [shared], fold_idx_info=fold_idx_info)\n",
    "\n",
    "\n",
    "class PatchOrderingLayer:\n",
    "    def __init__(self, size:tuple[int,int,int], patch:tuple[int,int]):\n",
    "        self.patch_fn = patchify(patch, patch)\n",
    "        self.size=size\n",
    "        self.patch=patch\n",
    "    \n",
    "    def __call__(self,x:torch.Tensor):\n",
    "        #x: (B,N) where N is W*H from the original image\n",
    "        #We first retrieve the original image\n",
    "        B,N = x.shape\n",
    "        x=x.reshape(B,*self.size)\n",
    "        patched = self.patch_fn(x)\n",
    "        return patched.reshape(B,N)\n",
    "\n",
    "# class TorchSharedParameter(TorchParameterInput):\n",
    "#     def __init__(self,\n",
    "#         in_shape:tuple[int,...],\n",
    "#         parameter:list[torch.nn.Module],\n",
    "#         num_folds:int\n",
    "# ):class TorchSharedParameter(TorchParameterInput):\n",
    "#     def __init__(self,\n",
    "#         in_shape:tuple[int,...],\n",
    "#         parameter:list[torch.nn.Module],\n",
    "#         num_folds:int\n",
    "# ):\n",
    "#         super().__init__()\n",
    "#         self._num_folds=num_folds\n",
    "#         self.in_shape=in_shape\n",
    "#         self.internal_param = torch.nn.ModuleList(parameter)\n",
    "    \n",
    "#     def forward(self):\n",
    "#         current_input=None\n",
    "#         for param in self.internal_param:\n",
    "#             if current_input is None:\n",
    "#                 current_input=param()\n",
    "#             else:\n",
    "#                 current_input=param(current_input)\n",
    "#         share_fold, *inner_units = current_input.shape\n",
    "#         return current_input.expand(self.num_folds//share_fold,share_fold, *inner_units).reshape(self.num_folds,*inner_units)\n",
    "\n",
    "#     @property\n",
    "#     def shape(self):\n",
    "#         return self.in_shape\n",
    "#         super().__init__()\n",
    "#         self._num_folds=num_folds\n",
    "#         self.in_shape=in_shape\n",
    "#         self.internal_param = torch.nn.ModuleList(parameter)\n",
    "    \n",
    "#     def forward(self):\n",
    "#         current_input=None\n",
    "#         for param in self.internal_param:\n",
    "#             if current_input is None:\n",
    "#                 current_input=param()\n",
    "#             else:\n",
    "#                 current_input=param(current_input)\n",
    "#         share_fold, *inner_units = current_input.shape\n",
    "#         return current_input.expand(self.num_folds//share_fold,share_fold, *inner_units).reshape(self.num_folds,*inner_units)\n",
    "\n",
    "#     @property\n",
    "#     def shape(self):\n",
    "#         return self.in_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf93f93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test LL: 669.779\n",
      "Bits per dimension: 1.233\n"
     ]
    }
   ],
   "source": [
    "from copy import copy\n",
    "from cirkit.symbolic.circuit import Scope\n",
    "from cirkit.symbolic.circuit import Circuit\n",
    "import sys\n",
    "sys.path.append(\"pconv\")\n",
    "\n",
    "from cirkit.symbolic.layers import InputLayer, SumLayer\n",
    "from cirkit.symbolic.parameters import (\n",
    "    Parameter,\n",
    ")\n",
    "from cirkit.backend.torch.parameters.parameter import TorchParameter\n",
    "from cirkit.backend.torch.parameters.nodes import TorchParameterInput\n",
    "\n",
    "from cirkit.backend.torch.layers.inner import TorchSumLayer\n",
    "from cirkit.backend.torch.layers.optimized import TorchCPTLayer\n",
    "from cirkit.backend.torch.layers.input import TorchInputLayer\n",
    "from cirkit.backend.torch.circuits import TorchCircuit\n",
    "from cirkit.backend.torch.parameters.nodes import TorchParameterInput, TorchMixingWeightParameter, TorchTensorParameter, TorchUnaryParameterOp\n",
    "from cirkit.backend.torch.parameters.parameter import TorchParameter\n",
    "from cirkit.backend.torch.layers.inner import TorchSumLayer\n",
    "from cirkit.backend.torch.layers.optimized import TorchCPTLayer, TorchTuckerLayer\n",
    "from cirkit.backend.torch.layers.input import TorchInputLayer\n",
    "from cirkit.backend.torch.circuits import TorchCircuit\n",
    "\n",
    "def patch_circuits(top_circuit_param, patch_circuit_param):\n",
    "    im_shape = [i*k for i,k in zip(top_circuit_param[\"image_shape\"],patch_circuit_param[\"image_shape\"])]\n",
    "    kernel_shape = patch_circuit_param[\"image_shape\"]\n",
    "\n",
    "    example_data = torch.arange(im_shape[0]*im_shape[1]*im_shape[2]).reshape(1, *im_shape)\n",
    "    patch_fn = patchify(kernel_shape[1:], kernel_shape[1:])\n",
    "    scope_order=patch_fn(example_data).reshape(-1, kernel_shape[1]*kernel_shape[2])\n",
    "    # print(example_data)\n",
    "    top = data_modalities.image_data(**top_circuit_param)\n",
    "    new_layers = top._nodes.copy()\n",
    "    new_inputs = top._in_nodes.copy()\n",
    "    for new_scope,input_node in zip(scope_order, list(top.layerwise_topological_ordering())[0]):\n",
    "        # Remove input node\n",
    "        new_layers.remove(input_node)\n",
    "        # add output of patch (create patch)\n",
    "        patch = data_modalities.image_data(**patch_circuit_param)\n",
    "        patch_input = list(patch.layerwise_topological_ordering())[0]\n",
    "        for idx,inp in enumerate(patch_input):\n",
    "            inp.scope= Scope([new_scope[list(inp.scope)[0]].item()])\n",
    "        new_layers.extend(patch._nodes)\n",
    "\n",
    "        # verify connections\n",
    "        for node, inputs in top._in_nodes.items():\n",
    "            if input_node in inputs:\n",
    "                new_inputs[node].remove(input_node)\n",
    "                new_inputs[node].extend(patch.outputs)\n",
    "        new_inputs.update(patch._in_nodes)\n",
    "        \n",
    "\n",
    "        \n",
    "    return Circuit(new_layers, new_inputs, top.outputs)\n",
    "    \n",
    "\n",
    "def copy_parameter(graph: TorchParameter, new_shape):\n",
    "    new_param_nodes = []\n",
    "    copy_map = {}\n",
    "    in_nodes = {}\n",
    "    outputs = []\n",
    "    for n in graph.topological_ordering():\n",
    "        instance = type(n)\n",
    "        config = n.config\n",
    "        if isinstance(n, TorchTensorParameter):\n",
    "            del config[\"shape\"]\n",
    "            new_param = instance(*new_shape,**config)\n",
    "            new_param._ptensor = torch.nn.Parameter(torch.zeros((graph.shape[0],*new_shape)))\n",
    "\n",
    "        elif isinstance(n, TorchUnaryParameterOp):\n",
    "            config[\"in_shape\"]=new_shape\n",
    "\n",
    "            new_param = instance(**config)\n",
    "        new_param_nodes.append(new_param)\n",
    "        copy_map[n] = new_param\n",
    "        inputs = [copy_map[in_node] for in_node in graph.node_inputs(n)]\n",
    "        if len(inputs) > 0:\n",
    "            in_nodes[new_param] = inputs\n",
    "    outputs = [copy_map[out_node] for out_node in graph.outputs]\n",
    "    parameter= TorchParameter(modules=new_param_nodes, in_modules=in_nodes, outputs=outputs)\n",
    "    return parameter\n",
    "\n",
    "class TorchSharedParameter(TorchParameterInput):\n",
    "    def __init__(self,\n",
    "        in_shape:tuple[int,...],\n",
    "        parameter:list[torch.nn.Module],\n",
    "        num_folds:int\n",
    "):\n",
    "        super().__init__()\n",
    "        self._num_folds=num_folds\n",
    "        self.in_shape=in_shape\n",
    "        self.internal_param = parameter\n",
    "    \n",
    "    def forward(self):\n",
    "        current_input=None\n",
    "        for param in self.internal_param:\n",
    "            if current_input is None:\n",
    "                current_input=param()\n",
    "            else:\n",
    "                current_input=param(current_input)\n",
    "        share_fold, *inner_units = current_input.shape\n",
    "        expanded = current_input.expand(self.num_folds//share_fold,share_fold, *inner_units).reshape(self.num_folds,*inner_units)\n",
    "\n",
    "        return expanded\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.in_shape\n",
    "\n",
    "def share_param_like(base_circ:TorchCircuit, share_struct:TorchCircuit, should_init_mean=False, should_freeze=False):\n",
    "    for idx, layer in enumerate(share_struct.layers):\n",
    "        if isinstance(layer, TorchInputLayer):\n",
    "            folds = base_circ.layers[idx].probs.num_folds\n",
    "            shared_param = TorchSharedParameter(base_circ.layers[idx].probs.shape, parameter=layer.probs.nodes, num_folds=folds)\n",
    "            base_circ.layers[idx].probs = shared_param\n",
    "        elif isinstance(layer, TorchCPTLayer) or isinstance(layer, TorchSumLayer):\n",
    "            internal_param=layer.weight.nodes\n",
    "            has_new_nodes=False\n",
    "            if layer.num_output_units != base_circ.layers[idx].num_output_units:\n",
    "                new_parameter = copy_parameter(layer.weight, base_circ.layers[idx].weight.nodes[0].shape)\n",
    "                _,o,i = internal_param[0]._ptensor.data.shape\n",
    "                _, goal_o, goal_i = new_parameter.nodes[0]._ptensor.data.shape\n",
    "                num_input = goal_i//i\n",
    "                num_output = goal_o//o\n",
    "                new_parameter.nodes[0]._ptensor.data=internal_param[0]._ptensor.data.clone().repeat((1, num_output, num_input))\n",
    "                has_new_nodes=True\n",
    "                internal_param=new_parameter.nodes\n",
    "            folds = base_circ.layers[idx].weight.num_folds\n",
    "            shared_param = TorchSharedParameter(base_circ.layers[idx].weight.shape, parameter=internal_param, num_folds=folds)\n",
    "            \n",
    "            if should_freeze and not has_new_nodes:\n",
    "                freeze_parameter(shared_param)\n",
    "            base_circ.layers[idx].weight = shared_param\n",
    "    if should_init_mean:\n",
    "        init_mean(base_circ, len(share_struct.layers))\n",
    "\n",
    "\n",
    "def init_mean(circ, start_idx):\n",
    "    for layer in circ.layers[start_idx:]:\n",
    "        if isinstance(layer, TorchCPTLayer) or isinstance(layer, TorchSumLayer):\n",
    "            param = layer.weight\n",
    "            tensor = param.nodes[0]._ptensor\n",
    "            inputs =tensor.shape[-1]\n",
    "\n",
    "            param.nodes[0]._ptensor.data = torch.full(tensor.shape, torch.exp(torch.tensor(1/inputs)))\n",
    "\n",
    "def freeze_parameter(param:TorchSharedParameter):\n",
    "    for p in param.parameters():\n",
    "        p.requires_grad = False\n",
    "            \n",
    "patch_units=512\n",
    "top_units=512\n",
    "new_circ=patch_circuits(\n",
    "    {\n",
    "        \"image_shape\": (1, 7,7),\n",
    "        \"region_graph\": \"quad-graph\",\n",
    "        \"input_layer\": \"categorical\",\n",
    "        \"num_input_units\": top_units,\n",
    "        \"sum_product_layer\": \"cp\",\n",
    "        \"num_sum_units\": top_units,\n",
    "        \"sum_weight_param\": utils.Parameterization(\n",
    "            activation=\"softmax\", initialization=\"normal\"\n",
    "        ),\n",
    "    },\n",
    "     {\n",
    "        \"image_shape\": (1, 4,4),\n",
    "        \"region_graph\": \"quad-graph\",\n",
    "        \"input_layer\": \"categorical\",\n",
    "        \"num_input_units\": patch_units,\n",
    "        \"sum_product_layer\": \"cp-t\",\n",
    "        \"num_sum_units\": patch_units,\n",
    "        \"num_classes\":top_units,\n",
    "        \"sum_weight_param\": utils.Parameterization(\n",
    "            activation=\"softmax\", initialization=\"normal\"\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from cirkit.pipeline import compile\n",
    "\n",
    "\n",
    "from src.benchmark_logic import BenchPCImage\n",
    "\n",
    "trained_mod = BenchPCImage.load_from_checkpoint(\"pconv/checkpoints/epoch=38-step=13728.ckpt\")\n",
    "patch_circ_trained = trained_mod.circuit\n",
    "patch_circ=data_modalities.image_data(\n",
    "        (1,4,4),\n",
    "        region_graph=\"quad-graph\",\n",
    "        input_layer=\"categorical\",\n",
    "        num_input_units=patch_units,\n",
    "        sum_product_layer=\"cp-t\",\n",
    "        num_sum_units=patch_units,\n",
    "        num_classes=1,\n",
    "        sum_weight_param=utils.Parameterization(\n",
    "            activation=\"softmax\", initialization=\"normal\"\n",
    "        ),\n",
    ")\n",
    "\n",
    "input_circ=data_modalities.image_data(\n",
    "        (1,1,1),\n",
    "        region_graph=\"quad-graph\",\n",
    "        input_layer=\"categorical\",\n",
    "        num_input_units=patch_units,\n",
    "        sum_product_layer=\"cp-t\",\n",
    "        num_sum_units=patch_units,\n",
    "        num_classes=patch_units,\n",
    "        sum_weight_param=utils.Parameterization(\n",
    "            activation=\"softmax\", initialization=\"normal\"\n",
    "        ),\n",
    ")\n",
    "cpatch = compile(patch_circ)\n",
    "cbase = compile(new_circ)\n",
    "# cinput = compile(input_circ)\n",
    "share_param_like(cbase, patch_circ_trained, should_freeze=True)\n",
    "# share_param_like(cbase, cinput)\n",
    "cbase\n",
    "test_circuit(cbase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72e53bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test LL: 669.779\n",
      "Bits per dimension: 1.233\n"
     ]
    }
   ],
   "source": [
    "test_circuit(patch_circ_trained, patch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7309167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big circuit: 920914946\n",
      "Shared circuit: 49822722\n",
      "Trained circuit: 2098178\n"
     ]
    }
   ],
   "source": [
    "pytorch_shared_params = sum(p.numel() for p in cbase.parameters() if p.requires_grad)\n",
    "old_big_circ = compile(base_circuit_factory(\"quad-graph\", \"cp\", 512))\n",
    "pytorch_total_params = sum(p.numel() for p in old_big_circ.parameters() if p.requires_grad)\n",
    "trained_params = sum(p.numel() for p in patch_circ_trained.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Big circuit:\", pytorch_total_params)\n",
    "print(\"Shared circuit:\",pytorch_shared_params)\n",
    "print(\"Trained circuit:\",trained_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7effe0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57166850"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_shared_params = sum(p.numel() for p in cbase.parameters() )\n",
    "pytorch_shared_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e99122",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2543f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86414589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d14cc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200: Average NLL: 640.526\n",
      "Step 400: Average NLL: 625.818\n",
      "Step 600: Average NLL: 621.707\n",
      "Step 800: Average NLL: 619.054\n",
      "Step 1000: Average NLL: 618.383\n",
      "Step 1200: Average NLL: 617.771\n",
      "Step 1400: Average NLL: 617.308\n",
      "Step 1600: Average NLL: 615.868\n",
      "Step 1800: Average NLL: 615.845\n",
      "Step 2000: Average NLL: 614.522\n",
      "Step 2200: Average NLL: 614.142\n",
      "Step 2400: Average NLL: 613.999\n",
      "Step 2600: Average NLL: 614.398\n",
      "Step 2800: Average NLL: 613.354\n",
      "Step 3000: Average NLL: 612.930\n",
      "Step 3200: Average NLL: 612.538\n",
      "Step 3400: Average NLL: 612.082\n",
      "Step 3600: Average NLL: 613.302\n",
      "Step 3800: Average NLL: 612.283\n",
      "Step 4000: Average NLL: 612.930\n",
      "Step 4200: Average NLL: 612.329\n",
      "Step 4400: Average NLL: 611.404\n",
      "Step 4600: Average NLL: 611.644\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([16, 784])\n",
      "torch.Size([16, 1, 1])\n",
      "Average test LL: 621.710\n",
      "Bits per dimension: 1.144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'# trainable parameters': 49822722,\n",
       " 'train loss': [640.5264282226562,\n",
       "  625.8182373046875,\n",
       "  621.7069702148438,\n",
       "  619.0540771484375,\n",
       "  618.382568359375,\n",
       "  617.7711791992188,\n",
       "  617.3084716796875,\n",
       "  615.86767578125,\n",
       "  615.8446044921875,\n",
       "  614.5216674804688,\n",
       "  614.141845703125,\n",
       "  613.9989624023438,\n",
       "  614.3976440429688,\n",
       "  613.353515625,\n",
       "  612.930419921875,\n",
       "  612.5384521484375,\n",
       "  612.0823974609375,\n",
       "  613.3015747070312,\n",
       "  612.2826538085938,\n",
       "  612.9298095703125,\n",
       "  612.3287353515625,\n",
       "  611.4038696289062,\n",
       "  611.6441040039062],\n",
       " 'test loss': 621.7103943359375,\n",
       " 'test bits per dimension': np.float64(1.1440542127265039),\n",
       " 'train loss (min)': 611.4038696289062,\n",
       " 'train time': 508.01440167427063,\n",
       " 'test time': 2.9679906368255615}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cirkit.pipeline import compile\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def train_and_eval_circuit(circuit, patch: bool):\n",
    "    # Set some seeds\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    # torch.cuda.manual_seed(42)\n",
    "\n",
    "    # Set the torch device to use\n",
    "    device = torch.device(DEVICE)\n",
    "    # device=\"cpu\"\n",
    "    # Move the circuit to chosen device\n",
    "    circuit = circuit.to(device)\n",
    "    patch_fn = patchify(KERNEL_SIZE, KERNEL_SIZE)\n",
    "    num_epochs = 20\n",
    "    step_idx = 0\n",
    "    running_loss = 0.0\n",
    "    running_samples = 0\n",
    "    stats = dict()\n",
    "\n",
    "    stats[\"# trainable parameters\"] = sum(\n",
    "        p.numel() for p in circuit.parameters() if p.requires_grad\n",
    "    )\n",
    "    stats[\"train loss\"] = []\n",
    "    patch_order=PatchOrderingLayer((1,*CIFAR_SIZE), KERNEL_SIZE)\n",
    "    # Initialize a torch optimizer of your choice,\n",
    "    #  e.g., Adam, by passing the parameters of the circuit\n",
    "    optimizer = torch.optim.Adam(circuit.parameters(), lr=0.05)\n",
    "    begin_train = time.time()\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        for i, (batch, _) in enumerate(train_dataloader):\n",
    "            # The circuit expects an input of shape (batch_dim, num_variables)\n",
    "            BS = batch.shape[0]\n",
    "            batch=batch.view(BS, -1)\n",
    "            if patch:\n",
    "                batch = patch_order(batch)\n",
    "            batch = batch.to(device)\n",
    "            # Compute the log-likelihoods of the batch, by evaluating the circuit\n",
    "            log_likelihoods = circuit(batch)\n",
    "\n",
    "            # We take the negated average log-likelihood as loss\n",
    "            loss = -torch.mean(log_likelihoods)\n",
    "            loss.backward()\n",
    "            # Update the parameters of the circuits, as any other model in PyTorch\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            running_loss += loss.detach() * len(batch)\n",
    "            running_samples += len(batch)\n",
    "            step_idx += 1\n",
    "            if step_idx % 200 == 0:\n",
    "                average_nll = running_loss / running_samples\n",
    "                print(f\"Step {step_idx}: Average NLL: {average_nll:.3f}\")\n",
    "                running_loss = 0.0\n",
    "                running_samples = 0\n",
    "\n",
    "                stats[\"train loss\"].append(average_nll.cpu().item())\n",
    "    end_train = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_lls = 0.0\n",
    "\n",
    "        for batch, _ in test_dataloader:\n",
    "            # The circuit expects an input of shape (batch_dim, num_variables)\n",
    "            BS = batch.shape[0]\n",
    "            batch=batch.view(BS, -1)\n",
    "            if patch:\n",
    "                batch = patch_order(batch)\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Compute the log-likelihoods of the batch\n",
    "            print(batch.shape)\n",
    "            log_likelihoods = circuit(batch)\n",
    "            print(log_likelihoods.shape)\n",
    "\n",
    "            # Accumulate the log-likelihoods\n",
    "            test_lls += log_likelihoods.sum().item()\n",
    "\n",
    "        # Compute average test log-likelihood and bits per dimension\n",
    "        average_nll = -test_lls / len(data_test)\n",
    "        bpd = average_nll / (28*28*1 * np.log(2.0))\n",
    "        print(f\"Average test LL: {average_nll:.3f}\")\n",
    "        print(f\"Bits per dimension: {bpd:.3f}\")\n",
    "        stats[\"test loss\"] = average_nll\n",
    "        stats[\"test bits per dimension\"] = bpd\n",
    "    end_test = time.time()\n",
    "\n",
    "    # Free GPU memory\n",
    "    circuit = circuit.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    stats[\"train loss (min)\"] = min(stats[\"train loss\"])\n",
    "    stats[\"train time\"] = end_train - begin_train\n",
    "    stats[\"test time\"] = end_test - end_train\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# results = dict()\n",
    "# for k, cc in circuits.items():\n",
    "#     print('\\nTraining circuit \"%s\"' % k)\n",
    "#     ctype = k.split(\"+\")[0].strip()\n",
    "#     results[k] = train_and_eval_circuit(cc, patch=False)\n",
    "#     results[k][\"type\"] = k.spli512t(\"+\")[0].strip()\n",
    "#     results[k][\"sum product layer\"] = k.split(\"+\")[1].strip()\n",
    "#     results[k][\"structure\"] = k.split(\"+\")[2].strip()\n",
    "# base_circ = cirkit_compile(stitched)\n",
    "# base_circ.reset_parameters()\n",
    "train_and_eval_circuit(cbase, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99ea2a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"pconv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2324db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.backend.torch.queries import SamplingQuery\n",
    "from src.utils import unpatchify\n",
    "\n",
    "unpatch_fn = unpatchify(CIFAR_SIZE, KERNEL_SIZE, KERNEL_SIZE,1)\n",
    "circuit = cbase.cpu()\n",
    "query = SamplingQuery(circuit)\n",
    "\n",
    "samples, _ = query(num_samples=1)\n",
    "# samples = unpatch_fn(samples).reshape(28,28,1)\n",
    "plt.imshow(samples.reshape(28,28,1), cmap=\"grey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c64cebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_circuit(circuit, patch=False):\n",
    "    device = torch.device(DEVICE)\n",
    "    circuit=circuit.to(device)\n",
    "    patch_fn = patchify(KERNEL_SIZE, KERNEL_SIZE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_lls = 0.0\n",
    "\n",
    "        for batch, _ in test_dataloader:\n",
    "            # The circuit expects an input of shape (batch_dim, num_variables)\n",
    "            if patch:\n",
    "                batch = patch_fn(batch)\n",
    "            BS = batch.shape[0]\n",
    "            batch = batch.view(BS, -1).to(device)\n",
    "\n",
    "            # Compute the log-likelihoods of the batch\n",
    "            log_likelihoods = circuit(batch)\n",
    "\n",
    "            # Accumulate the log-likelihoods\n",
    "            test_lls += log_likelihoods.sum().item()\n",
    "\n",
    "        # Compute average test log-likelihood and bits per dimension\n",
    "        average_nll = -test_lls / len(data_test)\n",
    "        bpd = average_nll / (28*28*1 * np.log(2.0))\n",
    "        print(f\"Average test LL: {average_nll:.3f}\")\n",
    "        print(f\"Bits per dimension: {bpd:.3f}\")\n",
    "# test_circuit(circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fbf46b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000]], grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cirkit.pipeline import integrate\n",
    "\n",
    "ic = integrate(circuit)\n",
    "ic(None).exp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
