{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63aed413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from cirkit.templates import data_modalities, utils\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "\n",
    "PIXEL_RANGE=255\n",
    "example_image = None\n",
    "\n",
    "KERNEL_SIZE=(4,4)\n",
    "CIFAR_SIZE=(28,28)\n",
    "DEVICE=\"cuda:6\"\n",
    "EPOCH=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b30dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cirkit.symbolic.layers import SumLayer, InputLayer\n",
    "from cirkit.symbolic.parameters import TensorParameter, ReferenceParameter\n",
    "from cirkit.symbolic.circuit import Circuit\n",
    "from cirkit.utils.scope import Scope\n",
    "from cirkit.symbolic.parameters import Parameter\n",
    "from copy import copy \n",
    "\n",
    "\n",
    "def copy_parameter(graph:Parameter):\n",
    "    new_param_nodes = []\n",
    "    copy_map = {}\n",
    "    in_nodes ={}\n",
    "    outputs=[]\n",
    "    for n in graph.topological_ordering():\n",
    "        instance = type(n)\n",
    "        if instance == TensorParameter:\n",
    "            new_param = ReferenceParameter(n) \n",
    "        else:\n",
    "            new_param = instance(**n.config)\n",
    "        new_param_nodes.append(new_param)\n",
    "        copy_map[n]=new_param\n",
    "        inputs = [copy_map[in_node] for in_node in graph.node_inputs(n)]\n",
    "        if len(inputs)>0:\n",
    "            in_nodes[new_param]=inputs\n",
    "    outputs = [copy_map[out_node] for out_node in graph.outputs]\n",
    "    return Parameter(nodes=new_param_nodes, in_nodes=in_nodes, outputs=outputs)\n",
    "\n",
    "def copy_circuit(graph:Circuit, scope_map:dict[int, int], root_node_outputs=None):\n",
    "    new_circ_layers = []\n",
    "    copy_map = {}\n",
    "    in_nodes ={}\n",
    "    outputs=[]\n",
    "    copied_params=[]\n",
    "    for layer in graph.topological_ordering():\n",
    "        instance = type(layer)\n",
    "        if isinstance(layer, SumLayer):\n",
    "            if layer.weight in copied_params:\n",
    "                parameter = copy_parameter(layer.weight)\n",
    "            else:\n",
    "                parameter=layer.weight\n",
    "                copied_params.append(layer.weight)\n",
    "            new_config = layer.config\n",
    "            new_layer = SumLayer(\n",
    "                **new_config,\n",
    "                weight = parameter\n",
    "                )\n",
    "        if isinstance(layer, InputLayer):\n",
    "            params = list(layer.params.items())\n",
    "            p_key = params[0][0]\n",
    "            p_graph = params[0][1]\n",
    "            new_scope = Scope([scope_map[s] for s in layer.scope])\n",
    "            config = copy(layer.config)\n",
    "            del config[\"scope\"]\n",
    "            if p_graph in copied_params:\n",
    "                new_p_graph = copy_parameter(p_graph)\n",
    "            else:\n",
    "                new_p_graph=p_graph\n",
    "                copied_params.append(p_graph)\n",
    "            new_layer = instance(scope= new_scope, **config, **{p_key:new_p_graph})\n",
    "        else:\n",
    "            new_config = layer.config\n",
    "            new_layer = instance(**new_config, **layer.params)\n",
    "        new_circ_layers.append(new_layer)\n",
    "        copy_map[layer]=new_layer\n",
    "        inputs = [copy_map[in_node] for in_node in graph.node_inputs(layer)]\n",
    "        if len(inputs)>0:\n",
    "            in_nodes[new_layer]=inputs\n",
    "    outputs = [copy_map[out_node] for out_node in graph.outputs ]\n",
    "    if root_node_outputs is not None:\n",
    "        for n in outputs:\n",
    "            n.num_output_units=root_node_outputs\n",
    "    return new_circ_layers, in_nodes,outputs\n",
    "\n",
    "def share_scope(big_circ:Circuit, share_small:Circuit, scope_size:int):\n",
    "    layers= copy(big_circ.nodes)\n",
    "    in_layers = copy(big_circ.nodes_inputs)\n",
    "    layers_to_replace = []\n",
    "    for n in big_circ.layerwise_topological_ordering():\n",
    "        if len(big_circ.layer_scope(n[0]))==scope_size and isinstance(n[0], SumLayer):\n",
    "            layers_to_replace = n\n",
    "            break\n",
    "\n",
    "    entry_points_map={}\n",
    "    for l in layers_to_replace:\n",
    "        for sl in big_circ.subgraph(l).topological_ordering():\n",
    "            to_remove=[sl]\n",
    "            while len(to_remove)>0:\n",
    "                tr = to_remove.pop()\n",
    "                if tr in in_layers:\n",
    "                    to_remove.extend(in_layers[tr])\n",
    "                    del in_layers[tr]\n",
    "                if tr in layers:\n",
    "                    layers.remove(tr)\n",
    "\n",
    "\n",
    "        scope_map=dict(zip(share_small.scope, big_circ.layer_scope(l)))\n",
    "        new_subgraph_layers, new_subgraph_inputs, new_output = copy_circuit(share_small, scope_map, root_node_outputs=sl.num_output_units)\n",
    "        layers.extend(new_subgraph_layers)\n",
    "        in_layers.update(new_subgraph_inputs)\n",
    "\n",
    "        entry_points_map[sl]=new_output\n",
    "\n",
    "    for old, new in entry_points_map.items():\n",
    "        for node, inputs in in_layers.items():\n",
    "            if old in inputs:\n",
    "                in_layers[node].remove(old)\n",
    "                in_layers[node].extend(new)\n",
    "\n",
    "\n",
    "    return Circuit(\n",
    "        layers = layers,\n",
    "        in_layers=in_layers,\n",
    "        outputs= big_circ.outputs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9fdfac",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb33551",
   "metadata": {},
   "source": [
    "Let's define a function to create and use patches of the base Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ab5373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(kernel_size, stride,compile=True, contiguous_output=False):\n",
    "    kh, kw = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
    "    sh, sw = (stride, stride) if isinstance(stride, int) else stride\n",
    "    def _patchify(image: torch.Tensor):\n",
    "        # Accept (C,H,W) or (B,C,H,W)\n",
    "\n",
    "        # Ensure contiguous NCHW for predictable strides\n",
    "        x = image.contiguous()  # (B,C,H,W)\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Number of patches along H/W\n",
    "        Lh = (H - kh) // sh + 1\n",
    "        Lw = (W - kw) // sw + 1\n",
    "\n",
    "        # Create a zero-copy view: (B, C, Lh, Lw, kh, kw)\n",
    "        sN, sC, sH, sW = x.stride()\n",
    "        patches = x.as_strided(\n",
    "            size=(B, C, Lh, Lw, kh, kw),\n",
    "            stride=(sN, sC, sH * sh, sW * sw, sH, sW),\n",
    "        )\n",
    "        # Reorder to (B, P, C, kh, kw) where P = Lh*Lw\n",
    "        patches = patches.permute(0, 2, 3, 1, 4, 5).reshape(B * Lh * Lw, C, kh, kw)\n",
    "\n",
    "        if contiguous_output:\n",
    "            patches = patches.contiguous()  # materialize if the next ops need contiguous\n",
    "\n",
    "        return patches\n",
    "    if compile:\n",
    "        _patchify = torch.compile(_patchify, fullgraph=True, dynamic=False)\n",
    "    return _patchify\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: (PIXEL_RANGE * x).long()),\n",
    "])\n",
    "\n",
    "data_train = datasets.MNIST('datasets', train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "# Instantiate the training and testing data loaders\n",
    "train_dataloader = DataLoader(data_train, shuffle=True, batch_size=256)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa33a031",
   "metadata": {},
   "source": [
    "## Defining the Circuit\n",
    "\n",
    "We want to create a factory to create the different circuit we will want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b2a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_circuit_factory(kernel_size, region_graph, layer_type, num_units, big_region_graph=None):\n",
    "    shared_circ= data_modalities.image_data(\n",
    "        (1,*kernel_size),\n",
    "        region_graph=region_graph,\n",
    "        input_layer=\"categorical\",\n",
    "        num_input_units=num_units,\n",
    "        sum_product_layer=layer_type,\n",
    "        num_sum_units=num_units,\n",
    "        num_classes=num_units,\n",
    "        sum_weight_param=utils.Parameterization(\n",
    "                activation='softmax',   \n",
    "                initialization='normal' \n",
    "            )\n",
    "    )\n",
    "    \n",
    "    big_circ= data_modalities.image_data(\n",
    "        (1,*CIFAR_SIZE),\n",
    "        region_graph=big_region_graph if big_region_graph is not None else region_graph,\n",
    "        input_layer=\"categorical\",\n",
    "        num_input_units=num_units,\n",
    "        sum_product_layer=layer_type,\n",
    "        num_sum_units=num_units,\n",
    "        sum_weight_param=utils.Parameterization(\n",
    "                activation='softmax',   \n",
    "                initialization='normal' \n",
    "            )\n",
    "    )\n",
    "\n",
    "    return share_scope(big_circ, shared_circ, math.prod(kernel_size))\n",
    "\n",
    "def base_circuit_factory(region_graph, layer_type, num_units):\n",
    "    return data_modalities.image_data(\n",
    "        (1,*CIFAR_SIZE),\n",
    "        region_graph=region_graph,\n",
    "        input_layer=\"categorical\",\n",
    "        num_input_units=num_units,\n",
    "        sum_product_layer=layer_type,\n",
    "        num_sum_units=num_units,\n",
    "        sum_weight_param=utils.Parameterization(\n",
    "                activation='softmax',   \n",
    "                initialization='normal' \n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a57e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits=dict()\n",
    "circuits[\"patch + cp.T + quad-graph\"] = patch_circuit_factory(KERNEL_SIZE, \"quad-graph\", \"cp-t\", 64)\n",
    "circuits[\"patch + cp.T + quad-tree-2\"] = patch_circuit_factory(KERNEL_SIZE, \"quad-tree-2\", \"cp-t\", 64)\n",
    "circuits[\"base + cp + quad-graph\"] = base_circuit_factory( \"quad-graph\", \"cp\", 64)\n",
    "circuits[\"base + tucker + quad-graph\"] = base_circuit_factory( \"quad-graph\", \"tucker\", 64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e99122",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86414589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d14cc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training circuit \"patch + cp.T + quad-graph\"\n",
      "Step 200: Average NLL: 1935.939\n",
      "Step 400: Average NLL: 878.520\n",
      "Step 600: Average NLL: 778.353\n",
      "Step 800: Average NLL: 739.160\n",
      "Step 1000: Average NLL: 717.448\n",
      "Step 1200: Average NLL: 702.763\n",
      "Step 1400: Average NLL: 691.142\n",
      "Step 1600: Average NLL: 681.267\n",
      "Step 1800: Average NLL: 675.247\n",
      "Step 2000: Average NLL: 668.037\n",
      "Step 2200: Average NLL: 663.195\n",
      "Step 2400: Average NLL: 659.029\n",
      "Step 2600: Average NLL: 656.097\n",
      "Step 2800: Average NLL: 651.511\n",
      "Step 3000: Average NLL: 648.038\n",
      "Step 3200: Average NLL: 644.693\n",
      "Step 3400: Average NLL: 642.142\n",
      "Step 3600: Average NLL: 640.662\n",
      "Step 3800: Average NLL: 638.053\n",
      "Step 4000: Average NLL: 635.680\n",
      "Step 4200: Average NLL: 633.034\n",
      "Step 4400: Average NLL: 630.434\n",
      "Step 4600: Average NLL: 628.678\n",
      "Step 4800: Average NLL: 626.955\n",
      "Step 5000: Average NLL: 625.747\n",
      "Step 5200: Average NLL: 624.266\n",
      "Step 5400: Average NLL: 622.264\n",
      "Step 5600: Average NLL: 620.642\n",
      "Step 5800: Average NLL: 619.380\n",
      "Step 6000: Average NLL: 618.142\n",
      "Step 6200: Average NLL: 617.148\n",
      "Step 6400: Average NLL: 617.136\n",
      "Step 6600: Average NLL: 615.688\n",
      "Step 6800: Average NLL: 614.433\n",
      "Step 7000: Average NLL: 613.754\n",
      "Average test LL: 688.880\n",
      "Bits per dimension: 1.268\n",
      "\n",
      "Training circuit \"patch + cp.T + quad-tree-2\"\n",
      "Step 200: Average NLL: 2616.978\n",
      "Step 400: Average NLL: 945.010\n",
      "Step 600: Average NLL: 820.211\n",
      "Step 800: Average NLL: 784.461\n",
      "Step 1000: Average NLL: 761.199\n",
      "Step 1200: Average NLL: 746.091\n",
      "Step 1400: Average NLL: 733.940\n",
      "Step 1600: Average NLL: 723.133\n",
      "Step 1800: Average NLL: 716.602\n",
      "Step 2000: Average NLL: 707.666\n",
      "Step 2200: Average NLL: 705.103\n",
      "Step 2400: Average NLL: 698.752\n",
      "Step 2600: Average NLL: 695.724\n",
      "Step 2800: Average NLL: 692.564\n",
      "Step 3000: Average NLL: 689.490\n",
      "Step 3200: Average NLL: 686.863\n",
      "Step 3400: Average NLL: 684.879\n",
      "Step 3600: Average NLL: 682.193\n",
      "Step 3800: Average NLL: 681.348\n",
      "Step 4000: Average NLL: 678.879\n",
      "Step 4200: Average NLL: 676.584\n",
      "Step 4400: Average NLL: 674.634\n",
      "Step 4600: Average NLL: 673.493\n",
      "Step 4800: Average NLL: 671.099\n",
      "Step 5000: Average NLL: 670.038\n",
      "Step 5200: Average NLL: 667.894\n",
      "Step 5400: Average NLL: 667.181\n",
      "Step 5600: Average NLL: 665.317\n",
      "Step 5800: Average NLL: 664.834\n",
      "Step 6000: Average NLL: 661.819\n",
      "Step 6200: Average NLL: 661.877\n",
      "Step 6400: Average NLL: 661.596\n",
      "Step 6600: Average NLL: 660.309\n",
      "Step 6800: Average NLL: 659.364\n",
      "Step 7000: Average NLL: 657.232\n",
      "Average test LL: 700.492\n",
      "Bits per dimension: 1.289\n",
      "\n",
      "Training circuit \"base + cp + quad-graph\"\n",
      "Step 200: Average NLL: 2492.184\n",
      "Step 400: Average NLL: 896.135\n",
      "Step 600: Average NLL: 787.980\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 107\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining circuit \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m k)\n\u001b[1;32m    106\u001b[0m ctype\u001b[38;5;241m=\u001b[39mk\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m--> 107\u001b[0m results[k] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_eval_circuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m results[k][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    109\u001b[0m results[k][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum product layer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "Cell \u001b[0;32mIn[13], line 39\u001b[0m, in \u001b[0;36mtrain_and_eval_circuit\u001b[0;34m(cc, patch)\u001b[0m\n\u001b[1;32m     37\u001b[0m begin_train\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (batch, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;66;03m# The circuit expects an input of shape (batch_dim, num_variables)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m patch:\n\u001b[1;32m     42\u001b[0m             batch\u001b[38;5;241m=\u001b[39mpatch_fn(batch)\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torchvision/datasets/mnist.py:139\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Any, Any]:\n\u001b[1;32m    132\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m        index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m        tuple: (image, target) where target is index of the target class.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     img \u001b[38;5;241m=\u001b[39m _Image_fromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from cirkit.pipeline import compile\n",
    "\n",
    "\n",
    "def train_and_eval_circuit(cc,patch:bool):\n",
    "\n",
    "    # Set some seeds\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    # torch.cuda.manual_seed(42)\n",
    "    \n",
    "    # Set the torch device to use\n",
    "    device = torch.device(DEVICE)\n",
    "    # device=\"cpu\"\n",
    "    # Move the circuit to chosen device\n",
    "    circuit = compile(cc)\n",
    "    circuit = circuit.to(device)\n",
    "    \n",
    "    num_epochs = EPOCH\n",
    "    step_idx = 0\n",
    "    running_loss = 0.0\n",
    "    running_samples = 0\n",
    "    stats = dict()\n",
    "\n",
    "    stats['# trainable parameters'] = sum(p.numel() for p in circuit.parameters() if p.requires_grad)\n",
    "    stats['train loss'] = []\n",
    "    patch_fn=patchify(KERNEL_SIZE, KERNEL_SIZE)\n",
    "    # Initialize a torch optimizer of your choice,\n",
    "    #  e.g., Adam, by passing the parameters of the circuit\n",
    "    optimizer = torch.optim.Adam(circuit.parameters(), lr=0.01)\n",
    "    begin_train=time.time()\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        for i, (batch, _) in enumerate(train_dataloader):\n",
    "            # The circuit expects an input of shape (batch_dim, num_variables)\n",
    "            if patch:\n",
    "                batch=patch_fn(batch)\n",
    "            BS = batch.shape[0]\n",
    "            batch = batch.view(BS, -1).to(device)\n",
    "    \n",
    "            # Compute the log-likelihoods of the batch, by evaluating the circuit\n",
    "            log_likelihoods = circuit(batch)\n",
    "    \n",
    "            # We take the negated average log-likelihood as loss\n",
    "            loss = -torch.mean(log_likelihoods)\n",
    "            loss.backward()\n",
    "            # Update the parameters of the circuits, as any other model in PyTorch\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            running_loss += loss.detach() * len(batch)\n",
    "            running_samples += len(batch)\n",
    "            step_idx += 1\n",
    "            if step_idx % 200 == 0:\n",
    "                average_nll = running_loss / running_samples\n",
    "                print(f\"Step {step_idx}: Average NLL: {average_nll:.3f}\")\n",
    "                running_loss = 0.0\n",
    "                running_samples = 0\n",
    "                \n",
    "                stats['train loss'].append(average_nll.cpu().item())\n",
    "    end_train=time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_lls = 0.0\n",
    "    \n",
    "        for batch, _ in test_dataloader:\n",
    "            # The circuit expects an input of shape (batch_dim, num_variables)\n",
    "            if patch:\n",
    "                batch=patch_fn(batch)\n",
    "            BS = batch.shape[0]\n",
    "            batch = batch.view(BS, -1).to(device)\n",
    "    \n",
    "            # Compute the log-likelihoods of the batch\n",
    "            log_likelihoods = circuit(batch)\n",
    "    \n",
    "            # Accumulate the log-likelihoods\n",
    "            test_lls += log_likelihoods.sum().item()\n",
    "    \n",
    "        # Compute average test log-likelihood and bits per dimension\n",
    "        average_nll = - test_lls / len(data_test)\n",
    "        bpd = average_nll / (28 * 28 * 1 * np.log(2.0))\n",
    "        print(f\"Average test LL: {average_nll:.3f}\")\n",
    "        print(f\"Bits per dimension: {bpd:.3f}\")\n",
    "        \n",
    "        stats['test loss'] = average_nll\n",
    "        stats['test bits per dimension'] = bpd\n",
    "    end_test=time.time()\n",
    "\n",
    "    # Free GPU memory\n",
    "    circuit = circuit.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    stats['train loss (min)'] = min(stats['train loss'])\n",
    "    stats['train time']= end_train - begin_train\n",
    "    stats['test time']= end_test - end_train\n",
    "\n",
    "    return stats\n",
    "\n",
    "results = dict()\n",
    "for k, cc in circuits.items():\n",
    "    print('\\nTraining circuit \"%s\"' % k)\n",
    "    ctype=k.split('+')[0].strip()\n",
    "    results[k] = train_and_eval_circuit(cc, patch=False)\n",
    "    results[k]['type'] = k.split('+')[0].strip()\n",
    "    results[k]['sum product layer'] = k.split('+')[1].strip()\n",
    "    results[k]['structure'] = k.split('+')[2].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fccac59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># trainable parameters</th>\n",
       "      <th>test loss</th>\n",
       "      <th>test bits per dimension</th>\n",
       "      <th>train loss (min)</th>\n",
       "      <th>train time</th>\n",
       "      <th>test time</th>\n",
       "      <th>type</th>\n",
       "      <th>sum product layer</th>\n",
       "      <th>structure</th>\n",
       "      <th>train time format</th>\n",
       "      <th>test time format</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>patch</th>\n",
       "      <td>38,163,586</td>\n",
       "      <td>688.880</td>\n",
       "      <td>1.268</td>\n",
       "      <td>613.754</td>\n",
       "      <td>427.826</td>\n",
       "      <td>1.420</td>\n",
       "      <td>patch</td>\n",
       "      <td>cp.T</td>\n",
       "      <td>quad-graph</td>\n",
       "      <td>01:07</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patch</th>\n",
       "      <td>16,048,192</td>\n",
       "      <td>700.492</td>\n",
       "      <td>1.289</td>\n",
       "      <td>657.232</td>\n",
       "      <td>317.867</td>\n",
       "      <td>1.494</td>\n",
       "      <td>patch</td>\n",
       "      <td>cp.T</td>\n",
       "      <td>quad-tree-2</td>\n",
       "      <td>01:17</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       # trainable parameters  test loss  test bits per dimension  \\\n",
       "patch              38,163,586    688.880                    1.268   \n",
       "patch              16,048,192    700.492                    1.289   \n",
       "\n",
       "        train loss (min)  train time  test time   type sum product layer  \\\n",
       "patch            613.754     427.826      1.420  patch              cp.T   \n",
       "patch            657.232     317.867      1.494  patch              cp.T   \n",
       "\n",
       "          structure train time format test time format  \n",
       "patch    quad-graph             01:07            01:01  \n",
       "patch   quad-tree-2             01:17            01:01  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(results, orient='index')\n",
    "df = df.drop(columns='train loss')\n",
    "\n",
    "df.index = df.index.map(lambda x: x.split('+')[0])\n",
    "df[\"# trainable parameters\"] = df[\"# trainable parameters\"].map('{:,d}'.format)\n",
    "pd.options.display.float_format = \"{:,.3f}\".format\n",
    "df[\"train time format\"]=pd.to_datetime(df['train time'], unit='s').dt.strftime(\"%m:%S\")\n",
    "df[\"test time format\"]=pd.to_datetime(df['test time'], unit='s').dt.strftime(\"%m:%S\")\n",
    "\n",
    "df.sort_values('test bits per dimension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c10e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllrrll}\n",
      "\\toprule\n",
      "type & sum product layer & structure & \\# trainable parameters & test bits per dimension & test loss & train time format & test time format \\\\\n",
      "\\midrule\n",
      "base & cp & quad-graph & 25,657,730 & 1.25 & 681.93 & 01:20 & 01:01 \\\\\n",
      "base & tucker & quad-graph & 421,306,626 & 1.26 & 683.16 & 01:50 & 01:02 \\\\\n",
      "base & cp & quad-tree-2 & 19,259,456 & 1.26 & 684.71 & 01:44 & 01:01 \\\\\n",
      "base & cp.T & quad-graph & 19,259,778 & 1.27 & 689.32 & 01:01 & 01:01 \\\\\n",
      "base & tucker & quad-tree-2 & 217,845,760 & 1.27 & 690.70 & 01:33 & 01:01 \\\\\n",
      "base & cp.T & quad-tree-2 & 16,048,192 & 1.28 & 693.99 & 01:38 & 01:01 \\\\\n",
      "patch & cp.T & quad-graph & 377,474 & 1.33 & 723.32 & 01:35 & 01:02 \\\\\n",
      "patch & cp.T & quad-tree-2 & 319,552 & 1.34 & 729.02 & 01:22 & 01:01 \\\\\n",
      "patch & cp & quad-graph & 508,546 & 1.35 & 731.46 & 01:34 & 01:01 \\\\\n",
      "patch & cp & quad-tree-2 & 385,088 & 1.35 & 734.02 & 01:23 & 01:01 \\\\\n",
      "patch & tucker & quad-graph & 7,610,882 & 1.35 & 734.18 & 01:10 & 01:02 \\\\\n",
      "patch & tucker & quad-tree-2 & 3,936,256 & 1.35 & 734.65 & 01:42 & 01:01 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df[[\"type\", \"sum product layer\", \"structure\",\"# trainable parameters\", \"test bits per dimension\", \"test loss\", \"train time format\", \"test time format\"]].sort_values('test bits per dimension').to_latex(\n",
    "    float_format=\"%.2f\",\n",
    "    escape=True,\n",
    "    index=False\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f59750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"bench_mnist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2324db16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f439c5a7e20>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cirkit.backend.torch.queries import SamplingQuery\n",
    "circuit = compile(circuits[\"patch + cp.T + quad-graph\"]).cpu()\n",
    "query = SamplingQuery(circuit)\n",
    "\n",
    "samples, _ = query(num_samples=1)\n",
    "img = samples.reshape((28,28,1))\n",
    "plt.imshow(img, cmap=\"grey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62dffe3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcircuit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodules\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "circuit.modules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
