{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63aed413",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from cirkit.templates import data_modalities, utils\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "PIXEL_RANGE = 255\n",
    "example_image = None\n",
    "\n",
    "KERNEL_SIZE = (4, 4)\n",
    "CIFAR_SIZE = (32,32)\n",
    "DEVICE = \"cuda:2\"\n",
    "EPOCH = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9fdfac",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb33551",
   "metadata": {},
   "source": [
    "Let's define a function to create and use patches of the base Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ab5373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(kernel_size, stride, compile=True, contiguous_output=False):\n",
    "    kh, kw = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
    "    sh, sw = (stride, stride) if isinstance(stride, int) else stride\n",
    "\n",
    "    def _patchify(image: torch.Tensor):\n",
    "        # Accept (C,H,W) or (B,C,H,W)\n",
    "\n",
    "        # Ensure contiguous NCHW for predictable strides\n",
    "        x = image.contiguous()  # (B,C,H,W)\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Number of patches along H/W\n",
    "        Lh = (H - kh) // sh + 1\n",
    "        Lw = (W - kw) // sw + 1\n",
    "\n",
    "        # Create a zero-copy view: (B, C, Lh, Lw, kh, kw)\n",
    "        sN, sC, sH, sW = x.stride()\n",
    "        patches = x.as_strided(\n",
    "            size=(B, C, Lh, Lw, kh, kw),\n",
    "            stride=(sN, sC, sH * sh, sW * sw, sH, sW),\n",
    "        )\n",
    "        # Reorder to (B, P, C, kh, kw) where P = Lh*Lw\n",
    "        patches = patches.permute(0, 2, 3, 1, 4, 5).reshape(B * Lh * Lw, C, kh, kw)\n",
    "\n",
    "        if contiguous_output:\n",
    "            patches = (\n",
    "                patches.contiguous()\n",
    "            )  # materialize if the next ops need contiguous\n",
    "\n",
    "        return patches\n",
    "\n",
    "    if compile:\n",
    "        _patchify = torch.compile(_patchify, fullgraph=True, dynamic=False)\n",
    "    return _patchify\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: (PIXEL_RANGE * x).long()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_train = datasets.CIFAR10(\"datasets\", train=True, download=True, transform=transform)\n",
    "data_test = datasets.CIFAR10(\"datasets\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Instantiate the training and testing data loaders\n",
    "train_dataloader = DataLoader(data_train, shuffle=True, drop_last=True, batch_size=256)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa33a031",
   "metadata": {},
   "source": [
    "## Defining the Circuit\n",
    "\n",
    "We want to create a factory to create the different circuit we will want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b2a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_circuit_factory(\n",
    "    kernel_size, region_graph, layer_type, num_units, big_region_graph=None\n",
    "):\n",
    "    shared_circ = data_modalities.image_data(\n",
    "        (3, *kernel_size),\n",
    "        region_graph=region_graph,\n",
    "        input_layer=\"categorical\",\n",
    "        num_input_units=num_units,\n",
    "        sum_product_layer=layer_type,\n",
    "        num_sum_units=num_units,\n",
    "        num_classes=num_units,\n",
    "        sum_weight_param=utils.Parameterization(\n",
    "            activation=\"softmax\", initialization=\"normal\"\n",
    "        ),\n",
    "    )\n",
    "    return shared_circ\n",
    "\n",
    "def top_circuit_factory(\n",
    "    subspace_size, region_graph, layer_type, num_units, big_region_graph=None\n",
    "):\n",
    "    shared_circ = data_modalities.image_data(\n",
    "        (3, *subspace_size),\n",
    "        region_graph=region_graph,\n",
    "        input_layer=\"categorical\",\n",
    "        num_input_units=num_units,\n",
    "        sum_product_layer=layer_type,\n",
    "        num_sum_units=num_units,\n",
    "        num_classes=1,\n",
    "        sum_weight_param=utils.Parameterization(\n",
    "            activation=\"softmax\", initialization=\"normal\"\n",
    "        ),\n",
    "    )\n",
    "    return shared_circ\n",
    "\n",
    "def base_circuit_factory(region_graph, layer_type, num_units):\n",
    "    return data_modalities.image_data(\n",
    "        (3, *CIFAR_SIZE),\n",
    "        # (1,*kernel_size),\n",
    "        region_graph=region_graph,\n",
    "        input_layer=\"categorical\",\n",
    "        num_input_units=num_units,\n",
    "        sum_product_layer=layer_type,\n",
    "        num_sum_units=num_units,\n",
    "        sum_weight_param=utils.Parameterization(\n",
    "            activation=\"softmax\", initialization=\"normal\"\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9089462f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from cirkit.backend.torch.layers.input import TorchCategoricalLayer\n",
    "from cirkit.backend.torch.parameters.parameter import TorchParameter, TorchParameterNode, FoldIndexInfo\n",
    "from cirkit.backend.torch.parameters.nodes import TorchParameterInput\n",
    "from cirkit.backend.torch.graph.folding import build_unfold_index_info\n",
    "from torch import Tensor\n",
    "from cirkit.backend.torch.semiring import Semiring\n",
    "from typing import Sequence, Mapping\n",
    "import functools\n",
    "\n",
    "\n",
    "def share_parameter(graph:TorchParameter, new_fold:int):\n",
    "    new_param_nodes = []\n",
    "    for i,n in enumerate(graph.topological_ordering()):\n",
    "        instance = type(n)\n",
    "        new_conf=n.config\n",
    "        new_conf[\"num_folds\"]=new_fold\n",
    "        if \"initializer_\" in new_conf:\n",
    "            reduced_initializer_list = new_conf[\"initializer_\"].keywords[\"initializers\"][:new_fold]\n",
    "            new_conf[\"initializer_\"]= functools.partial(new_conf[\"initializer_\"].func, initializers=reduced_initializer_list)\n",
    "        if \"shape\" in new_conf:\n",
    "            shape=new_conf[\"shape\"]\n",
    "            del new_conf[\"shape\"]\n",
    "            new_param = instance(*shape, **new_conf)\n",
    "        else:\n",
    "            new_param = instance( **new_conf)\n",
    "        new_param.reset_parameters()\n",
    "        new_param_nodes.append(new_param)\n",
    "   \n",
    "    shared= TorchSharedParameter(graph.shape, new_param_nodes, num_folds=graph.num_folds)\n",
    "    fold_idx_info = FoldIndexInfo(\n",
    "        ordering=[shared],\n",
    "        in_fold_idx={0:[[]]},\n",
    "        out_fold_idx=[(0, f) for f in range(graph.num_folds)]\n",
    "    )\n",
    "    return TorchParameter([shared], {shared:[]}, [shared], fold_idx_info=fold_idx_info)\n",
    "\n",
    "\n",
    "class PatchOrderingLayer:\n",
    "    def __init__(self, size:tuple[int,int,int], patch:tuple[int,int]):\n",
    "        self.patch_fn = patchify(patch, patch)\n",
    "        self.size=size\n",
    "        self.patch=patch\n",
    "    \n",
    "    def __call__(self,x:torch.Tensor):\n",
    "        #x: (B,N) where N is W*H from the original image\n",
    "        #We first retrieve the original image\n",
    "        B,N = x.shape\n",
    "        x=x.reshape(B,*self.size)\n",
    "        patched = self.patch_fn(x)\n",
    "        return patched.reshape(B,N)\n",
    "\n",
    "# class TorchSharedParameter(TorchParameterInput):\n",
    "#     def __init__(self,\n",
    "#         in_shape:tuple[int,...],\n",
    "#         parameter:list[torch.nn.Module],\n",
    "#         num_folds:int\n",
    "# ):class TorchSharedParameter(TorchParameterInput):\n",
    "#     def __init__(self,\n",
    "#         in_shape:tuple[int,...],\n",
    "#         parameter:list[torch.nn.Module],\n",
    "#         num_folds:int\n",
    "# ):\n",
    "#         super().__init__()\n",
    "#         self._num_folds=num_folds\n",
    "#         self.in_shape=in_shape\n",
    "#         self.internal_param = torch.nn.ModuleList(parameter)\n",
    "    \n",
    "#     def forward(self):\n",
    "#         current_input=None\n",
    "#         for param in self.internal_param:\n",
    "#             if current_input is None:\n",
    "#                 current_input=param()\n",
    "#             else:\n",
    "#                 current_input=param(current_input)\n",
    "#         share_fold, *inner_units = current_input.shape\n",
    "#         return current_input.expand(self.num_folds//share_fold,share_fold, *inner_units).reshape(self.num_folds,*inner_units)\n",
    "\n",
    "#     @property\n",
    "#     def shape(self):\n",
    "#         return self.in_shape\n",
    "#         super().__init__()\n",
    "#         self._num_folds=num_folds\n",
    "#         self.in_shape=in_shape\n",
    "#         self.internal_param = torch.nn.ModuleList(parameter)\n",
    "    \n",
    "#     def forward(self):\n",
    "#         current_input=None\n",
    "#         for param in self.internal_param:\n",
    "#             if current_input is None:\n",
    "#                 current_input=param()\n",
    "#             else:\n",
    "#                 current_input=param(current_input)\n",
    "#         share_fold, *inner_units = current_input.shape\n",
    "#         return current_input.expand(self.num_folds//share_fold,share_fold, *inner_units).reshape(self.num_folds,*inner_units)\n",
    "\n",
    "#     @property\n",
    "#     def shape(self):\n",
    "#         return self.in_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf93f93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    1,    2,  ..., 2145, 2146, 2147],\n",
      "        [   4,    5,    6,  ..., 2149, 2150, 2151],\n",
      "        [   8,    9,   10,  ..., 2153, 2154, 2155],\n",
      "        ...,\n",
      "        [ 916,  917,  918,  ..., 3061, 3062, 3063],\n",
      "        [ 920,  921,  922,  ..., 3065, 3066, 3067],\n",
      "        [ 924,  925,  926,  ..., 3069, 3070, 3071]])\n",
      "{'circuit_type': 'patch', 'layer_type': 'cp', 'region_graph': 'quad-graph', 'num_units': 512, 'lr': 0.05, 'dataset': 'cifar', 'kernel_size': [4, 4], 'batch_size': 128, 'experiment_path': 'experiments/bench-high-conf/', 'image_size': (32, 32), 'channel': 3}\n",
      "TorchCategoricalLayer(\n",
      "  folds: 48  variables: 1  output-units: 512\n",
      "  input-shape: (48, 1, -1, 1)\n",
      "  output-shape: (48, -1, 512)\n",
      "  (probs): TorchParameter(\n",
      "    shape: (48, 512, 256)\n",
      "    (0): TorchTensorParameter(output-shape: (48, 512, 256))\n",
      "    (1): TorchSoftmaxParameter(\n",
      "      input-shapes: [(48, 512, 256)]\n",
      "      output-shape: (48, 512, 256)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "TorchHadamardLayer(\n",
      "  folds: 16  arity: 3  input-units: 512  output-units: 512\n",
      "  input-shape: (16, 3, -1, 512)\n",
      "  output-shape: (16, -1, 512)\n",
      ")\n",
      "TorchSumLayer(\n",
      "  folds: 32  arity: 1  input-units: 512  output-units: 512\n",
      "  input-shape: (32, 1, -1, 512)\n",
      "  output-shape: (32, -1, 512)\n",
      "  (weight): TorchParameter(\n",
      "    shape: (32, 512, 512)\n",
      "    (0): TorchTensorParameter(output-shape: (32, 512, 512))\n",
      "    (1): TorchSoftmaxParameter(\n",
      "      input-shapes: [(32, 512, 512)]\n",
      "      output-shape: (32, 512, 512)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "TorchCPTLayer(\n",
      "  folds: 16  arity: 2  input-units: 512  output-units: 512\n",
      "  input-shape: (16, 2, -1, 512)\n",
      "  output-shape: (16, -1, 512)\n",
      "  (weight): TorchParameter(\n",
      "    shape: (16, 512, 512)\n",
      "    (0): TorchTensorParameter(output-shape: (16, 512, 512))\n",
      "    (1): TorchSoftmaxParameter(\n",
      "      input-shapes: [(16, 512, 512)]\n",
      "      output-shape: (16, 512, 512)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "TorchHadamardLayer(\n",
      "  folds: 8  arity: 2  input-units: 512  output-units: 512\n",
      "  input-shape: (8, 2, -1, 512)\n",
      "  output-shape: (8, -1, 512)\n",
      ")\n",
      "TorchSumLayer(\n",
      "  folds: 4  arity: 2  input-units: 512  output-units: 512\n",
      "  input-shape: (4, 2, -1, 512)\n",
      "  output-shape: (4, -1, 512)\n",
      "  (weight): TorchParameter(\n",
      "    shape: (4, 512, 1024)\n",
      "    (0): TorchTensorParameter(output-shape: (4, 512, 2))\n",
      "    (1): TorchSoftmaxParameter(\n",
      "      input-shapes: [(4, 512, 2)]\n",
      "      output-shape: (4, 512, 2)\n",
      "    )\n",
      "    (2): TorchMixingWeightParameter(\n",
      "      input-shapes: [(4, 512, 2)]\n",
      "      output-shape: (4, 512, 1024)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "TorchSumLayer(\n",
      "  folds: 8  arity: 1  input-units: 512  output-units: 512\n",
      "  input-shape: (8, 1, -1, 512)\n",
      "  output-shape: (8, -1, 512)\n",
      "  (weight): TorchParameter(\n",
      "    shape: (8, 512, 512)\n",
      "    (0): TorchTensorParameter(output-shape: (8, 512, 512))\n",
      "    (1): TorchSoftmaxParameter(\n",
      "      input-shapes: [(8, 512, 512)]\n",
      "      output-shape: (8, 512, 512)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "TorchCPTLayer(\n",
      "  folds: 4  arity: 2  input-units: 512  output-units: 512\n",
      "  input-shape: (4, 2, -1, 512)\n",
      "  output-shape: (4, -1, 512)\n",
      "  (weight): TorchParameter(\n",
      "    shape: (4, 512, 512)\n",
      "    (0): TorchTensorParameter(output-shape: (4, 512, 512))\n",
      "    (1): TorchSoftmaxParameter(\n",
      "      input-shapes: [(4, 512, 512)]\n",
      "      output-shape: (4, 512, 512)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "TorchCPTLayer(\n",
      "  folds: 2  arity: 2  input-units: 512  output-units: 1\n",
      "  input-shape: (2, 2, -1, 512)\n",
      "  output-shape: (2, -1, 1)\n",
      "  (weight): TorchParameter(\n",
      "    shape: (2, 1, 512)\n",
      "    (0): TorchTensorParameter(output-shape: (2, 1, 512))\n",
      "    (1): TorchSoftmaxParameter(\n",
      "      input-shapes: [(2, 1, 512)]\n",
      "      output-shape: (2, 1, 512)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "torch.Size([2, 1, 512])\n",
      "torch.Size([1, 512, 512])\n",
      "torch.Size([2, 512, 512])\n",
      "TorchSumLayer(\n",
      "  folds: 1  arity: 2  input-units: 1  output-units: 1\n",
      "  input-shape: (1, 2, -1, 1)\n",
      "  output-shape: (1, -1, 1)\n",
      "  (weight): TorchParameter(\n",
      "    shape: (1, 1, 2)\n",
      "    (0): TorchTensorParameter(output-shape: (1, 1, 2))\n",
      "    (1): TorchSoftmaxParameter(\n",
      "      input-shapes: [(1, 1, 2)]\n",
      "      output-shape: (1, 1, 2)\n",
      "    )\n",
      "    (2): TorchMixingWeightParameter(\n",
      "      input-shapes: [(1, 1, 2)]\n",
      "      output-shape: (1, 1, 2)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 1, 2])\n",
      "torch.Size([1, 512, 2])\n",
      "torch.Size([1, 512, 2])\n"
     ]
    }
   ],
   "source": [
    "from copy import copy\n",
    "from cirkit.symbolic.circuit import Scope\n",
    "from cirkit.symbolic.circuit import Circuit\n",
    "import gc\n",
    "\n",
    "from cirkit.symbolic.layers import InputLayer, SumLayer\n",
    "from cirkit.symbolic.parameters import (\n",
    "    Parameter,\n",
    ")\n",
    "from cirkit.backend.torch.parameters.parameter import TorchParameter\n",
    "from cirkit.backend.torch.parameters.nodes import TorchParameterInput\n",
    "\n",
    "from cirkit.backend.torch.layers.inner import TorchSumLayer\n",
    "from cirkit.backend.torch.layers.optimized import TorchCPTLayer\n",
    "from cirkit.backend.torch.layers.input import TorchInputLayer\n",
    "from cirkit.backend.torch.circuits import TorchCircuit\n",
    "from cirkit.backend.torch.parameters.nodes import TorchParameterInput, TorchMixingWeightParameter, TorchTensorParameter, TorchUnaryParameterOp\n",
    "from cirkit.backend.torch.parameters.parameter import TorchParameter\n",
    "from cirkit.backend.torch.layers.inner import TorchSumLayer\n",
    "from cirkit.backend.torch.layers.optimized import TorchCPTLayer, TorchTuckerLayer\n",
    "from cirkit.backend.torch.layers.input import TorchInputLayer\n",
    "from cirkit.backend.torch.circuits import TorchCircuit\n",
    "\n",
    "def patch_circuits(top_circuit_param, patch_circuit_param):\n",
    "    im_shape = [i*k for i,k in zip(top_circuit_param[\"image_shape\"],patch_circuit_param[\"image_shape\"])]\n",
    "    kernel_shape = patch_circuit_param[\"image_shape\"]\n",
    "\n",
    "    example_data = torch.arange(im_shape[0]*im_shape[1]*im_shape[2]).reshape(1, *im_shape)\n",
    "    patch_fn = patchify(kernel_shape[1:], kernel_shape[1:])\n",
    "    scope_order=patch_fn(example_data).reshape(-1, kernel_shape[0]*kernel_shape[1]*kernel_shape[2])\n",
    "    print(scope_order)\n",
    "    top = data_modalities.image_data(**top_circuit_param)\n",
    "    new_layers = top._nodes.copy()\n",
    "    new_inputs = top._in_nodes.copy()\n",
    "    for new_scope,input_node in zip(scope_order, list(top.layerwise_topological_ordering())[0]):\n",
    "        # Remove input node\n",
    "        new_layers.remove(input_node)\n",
    "        # add output of patch (create patch)\n",
    "        patch = data_modalities.image_data(**patch_circuit_param)\n",
    "        patch_input = list(patch.layerwise_topological_ordering())[0]\n",
    "        for idx,inp in enumerate(patch_input):\n",
    "            inp.scope= Scope([new_scope[list(inp.scope)[0]].item()])\n",
    "        new_layers.extend(patch._nodes)\n",
    "\n",
    "        # verify connections\n",
    "        for node, inputs in top._in_nodes.items():\n",
    "            if input_node in inputs:\n",
    "                new_inputs[node].remove(input_node)\n",
    "                new_inputs[node].extend(patch.outputs)\n",
    "        new_inputs.update(patch._in_nodes)\n",
    "        \n",
    "\n",
    "        \n",
    "    return Circuit(new_layers, new_inputs, top.outputs)\n",
    "\n",
    "\n",
    "def copy_parameter(graph: TorchParameter, new_shape):\n",
    "    new_param_nodes = []\n",
    "    copy_map = {}\n",
    "    in_nodes = {}\n",
    "    outputs = []\n",
    "    for n in graph.topological_ordering():\n",
    "        instance = type(n)\n",
    "        config = n.config\n",
    "        if isinstance(n, TorchTensorParameter):\n",
    "            del config[\"shape\"]\n",
    "            new_param = instance(*new_shape,**config)\n",
    "            new_param._ptensor = torch.nn.Parameter(torch.zeros((graph.shape[0],*new_shape)))\n",
    "\n",
    "        elif isinstance(n, TorchUnaryParameterOp):\n",
    "            config[\"in_shape\"]=new_shape\n",
    "\n",
    "            new_param = instance(**config)\n",
    "        new_param_nodes.append(new_param)\n",
    "        copy_map[n] = new_param\n",
    "        inputs = [copy_map[in_node] for in_node in graph.node_inputs(n)]\n",
    "        if len(inputs) > 0:\n",
    "            in_nodes[new_param] = inputs\n",
    "    outputs = [copy_map[out_node] for out_node in graph.outputs]\n",
    "    parameter= TorchParameter(modules=new_param_nodes, in_modules=in_nodes, outputs=outputs)\n",
    "    return parameter\n",
    "\n",
    "class TorchSharedParameter(TorchParameterInput):\n",
    "    def __init__(self,\n",
    "        in_shape:tuple[int,...],\n",
    "        parameter:list[torch.nn.Module],\n",
    "        num_folds:int\n",
    "):\n",
    "        super().__init__()\n",
    "        self._num_folds=num_folds\n",
    "        self.in_shape=in_shape\n",
    "        self.internal_param = parameter\n",
    "    \n",
    "    def forward(self):\n",
    "        current_input=None\n",
    "        for param in self.internal_param:\n",
    "            if current_input is None:\n",
    "                current_input=param()\n",
    "            else:\n",
    "                current_input=param(current_input)\n",
    "        share_fold, *inner_units = current_input.shape\n",
    "        expanded = current_input.expand(self.num_folds//share_fold,share_fold, *inner_units).reshape(self.num_folds,*inner_units)\n",
    "\n",
    "        return expanded\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.in_shape\n",
    "\n",
    "def share_param_like(base_circ:TorchCircuit, share_struct:TorchCircuit, should_init_mean=False):\n",
    "    for idx, layer in enumerate(share_struct.layers):\n",
    "        print(layer)\n",
    "        if isinstance(layer, TorchInputLayer):\n",
    "            \n",
    "            folds = base_circ.layers[idx].probs.num_folds\n",
    "            shared_param = TorchSharedParameter(base_circ.layers[idx].probs.shape, parameter=layer.probs.nodes, num_folds=folds)\n",
    "            base_circ.layers[idx].probs = shared_param\n",
    "        elif isinstance(layer, TorchCPTLayer) or isinstance(layer, TorchSumLayer):\n",
    "            internal_param=layer.weight.nodes\n",
    "            if layer.num_output_units != base_circ.layers[idx].num_output_units:\n",
    "                new_parameter = copy_parameter(layer.weight, base_circ.layers[idx].weight.nodes[0].shape)\n",
    "                print(internal_param[0]._ptensor.data.shape)\n",
    "\n",
    "                print(new_parameter.nodes[0]._ptensor.data.shape)\n",
    "                _,o,i = internal_param[0]._ptensor.data.shape\n",
    "                _, goal_o, goal_i = new_parameter.nodes[0]._ptensor.data.shape\n",
    "                num_input = goal_i//i\n",
    "                num_output = goal_o//o\n",
    "                new_parameter.nodes[0]._ptensor.data=internal_param[0]._ptensor.data.clone().repeat((1, num_output, num_input))\n",
    "                print(new_parameter.nodes[0]._ptensor.data.shape)\n",
    "\n",
    "                internal_param=new_parameter.nodes\n",
    "            folds = base_circ.layers[idx].weight.num_folds\n",
    "            shared_param = TorchSharedParameter(base_circ.layers[idx].weight.shape, parameter=internal_param, num_folds=folds)\n",
    "            base_circ.layers[idx].weight = shared_param\n",
    "    if should_init_mean:\n",
    "        init_mean(base_circ, len(share_struct.layers))\n",
    "\n",
    "\n",
    "def init_mean(circ, start_idx):\n",
    "    for layer in circ.layers[start_idx:]:\n",
    "        print(layer)\n",
    "        if isinstance(layer, TorchCPTLayer) or isinstance(layer, TorchSumLayer):\n",
    "            param = layer.weight\n",
    "            tensor = param.nodes[0]._ptensor\n",
    "            inputs =tensor.shape[-1]\n",
    "\n",
    "            param.nodes[0]._ptensor.data = torch.full(tensor.shape, torch.exp(torch.tensor(1/inputs)))\n",
    "\n",
    "\n",
    "def freeze_parameter(param:TorchSharedParameter):\n",
    "    for p in param.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "def rescope(base_circ, patch_circ, kernel_shape, im_shape):\n",
    "    example_data = torch.arange(im_shape[0]*im_shape[1]*im_shape[2]).reshape(1, *im_shape)\n",
    "    patch_fn = patchify(kernel_shape[1:], kernel_shape[1:])\n",
    "\n",
    "    scope_order=patch_fn(example_data).reshape(-1,  kernel_shape[0]*kernel_shape[1]*kernel_shape[2])\n",
    "\n",
    "    new_order=torch.empty_like(base_circ.layers[0].scope_idx)\n",
    "    print(new_order.shape)\n",
    "    for idx, patch_scope in enumerate(scope_order):\n",
    "        start=idx*len(patch_scope)\n",
    "        end=start+len(patch_scope)\n",
    "        new_order[start:end]=patch_scope[patch_circ.layers[0].scope_idx]\n",
    "    base_circ.layers[0]._scope_idx=new_order\n",
    "    # return new_order\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "patch_units=512\n",
    "top_units=512\n",
    "new_circ=patch_circuits(\n",
    "    {\n",
    "        \"image_shape\": (1, 8,8),\n",
    "        \"region_graph\": \"quad-graph\",\n",
    "        \"input_layer\": \"categorical\",\n",
    "        \"num_input_units\": top_units,\n",
    "        \"sum_product_layer\": \"cp\",\n",
    "        \"num_sum_units\": top_units,\n",
    "        \"sum_weight_param\": utils.Parameterization(\n",
    "            activation=\"softmax\", initialization=\"normal\"\n",
    "        ),\n",
    "    },\n",
    "     {\n",
    "        \"image_shape\": (3,4,4),\n",
    "        \"region_graph\": \"quad-graph\",\n",
    "        \"input_layer\": \"categorical\",\n",
    "        \"num_input_units\": patch_units,\n",
    "        \"sum_product_layer\": \"cp\",\n",
    "        \"num_sum_units\": patch_units,\n",
    "        \"num_classes\":top_units,\n",
    "        \"sum_weight_param\": utils.Parameterization(\n",
    "            activation=\"softmax\", initialization=\"normal\"\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from cirkit.pipeline import compile\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"pconv\")\n",
    "\n",
    "from src.benchmark_logic import BenchPCImage\n",
    "\n",
    "trained_mod = BenchPCImage.load_from_checkpoint(\"pconv/checkpoints/epoch=99-step=29300.ckpt\").cpu()\n",
    "print(trained_mod.config)\n",
    "patch_circ_trained = trained_mod.circuit\n",
    "\n",
    "patch_circ=data_modalities.image_data(\n",
    "        (3,4,4),\n",
    "        region_graph=\"quad-graph\",\n",
    "        input_layer=\"categorical\",\n",
    "        num_input_units=patch_units,\n",
    "        sum_product_layer=\"cp\",\n",
    "        num_sum_units=patch_units,\n",
    "        num_classes=1,\n",
    "        sum_weight_param=utils.Parameterization(\n",
    "            activation=\"softmax\", initialization=\"normal\"\n",
    "        ),\n",
    ")\n",
    "# input_circ=data_modalities.image_data(\n",
    "#         (3,1,1),\n",
    "#         region_graph=\"quad-graph\",\n",
    "#         input_layer=\"categorical\",\n",
    "#         num_input_units=patch_units,\n",
    "#         sum_product_layer=\"cp-t\",\n",
    "#         num_sum_units=patch_units,\n",
    "#         num_classes=patch_units,\n",
    "#         sum_weight_param=utils.Parameterization(\n",
    "#             activation=\"softmax\", initialization=\"normal\"\n",
    "#         ),\n",
    "# )\n",
    "cpatch = compile(patch_circ)\n",
    "cbase = compile(new_circ)\n",
    "# cinput = compile(input_circ)\n",
    "\n",
    "share_param_like(cbase, patch_circ_trained)\n",
    "# rescope(cbase, patch_circ_trained, (3,4,4), (3,32,32))\n",
    "# share_param_like(cbase, cinput)\n",
    "# cbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7309167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1475695618\n",
      "88631298\n"
     ]
    }
   ],
   "source": [
    "pytorch_shared_params = sum(p.numel() for p in cbase.parameters() if p.requires_grad)\n",
    "old_big_circ = compile(base_circuit_factory(\"quad-graph\", \"cp\", 512))\n",
    "pytorch_total_params = sum(p.numel() for p in old_big_circ.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)\n",
    "print(pytorch_shared_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30fd63ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test LL: 11710.888\n",
      "Bits per dimension: 5.500\n"
     ]
    }
   ],
   "source": [
    "def test_circuit(circuit, patch=False):\n",
    "    device = torch.device(DEVICE)\n",
    "    circuit=circuit.to(device)\n",
    "    patch_fn = patchify(KERNEL_SIZE, KERNEL_SIZE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_lls = 0.0\n",
    "\n",
    "        for batch, _ in test_dataloader:\n",
    "            # The circuit expects an input of shape (batch_dim, num_variables)\n",
    "            if patch:\n",
    "                batch = patch_fn(batch)\n",
    "            BS = batch.shape[0]\n",
    "            batch = batch.view(BS, -1).to(device)\n",
    "\n",
    "            # Compute the log-likelihoods of the batch\n",
    "            log_likelihoods = circuit(batch)\n",
    "\n",
    "            # Accumulate the log-likelihoods\n",
    "            test_lls += log_likelihoods.sum().item()\n",
    "\n",
    "        # Compute average test log-likelihood and bits per dimension\n",
    "        average_nll = -test_lls / len(data_test)\n",
    "        bpd = average_nll / (32*32*3 * np.log(2.0))\n",
    "        print(f\"Average test LL: {average_nll:.3f}\")\n",
    "        print(f\"Bits per dimension: {bpd:.3f}\")\n",
    "\n",
    "test_circuit(cbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cf6d9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test LL: 11710.888\n",
      "Bits per dimension: 5.500\n"
     ]
    }
   ],
   "source": [
    "test_circuit(patch_circ_trained, patch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "637a4121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-24592.3906]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randint(256, (1, 3,32,32))\n",
    "cbase.cpu()\n",
    "cbase(data.reshape(-1,3*32*32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20ce76f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-24592.3906, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_fn = patchify(4, 4)\n",
    "patched = patch_fn(data).reshape(-1,3*4*4).cpu()\n",
    "patch_circ_trained.cpu()\n",
    "patch_circ_trained(patched).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3762804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0116, 0.0033, 0.0048,  ..., 0.0011, 0.0020, 0.0005],\n",
       "         [0.0008, 0.0010, 0.0009,  ..., 0.0009, 0.0039, 0.0018],\n",
       "         [0.0026, 0.0023, 0.0004,  ..., 0.0013, 0.0220, 0.0153],\n",
       "         ...,\n",
       "         [0.0014, 0.0025, 0.0007,  ..., 0.0040, 0.0026, 0.0078],\n",
       "         [0.0026, 0.0010, 0.0092,  ..., 0.0038, 0.0009, 0.0006],\n",
       "         [0.0077, 0.0009, 0.0019,  ..., 0.0046, 0.0067, 0.0046]],\n",
       "\n",
       "        [[0.0085, 0.0005, 0.0011,  ..., 0.0014, 0.0112, 0.0040],\n",
       "         [0.0014, 0.0100, 0.0075,  ..., 0.0009, 0.0008, 0.0012],\n",
       "         [0.0021, 0.0035, 0.0036,  ..., 0.0021, 0.0023, 0.0007],\n",
       "         ...,\n",
       "         [0.0022, 0.0016, 0.0045,  ..., 0.0031, 0.0019, 0.0026],\n",
       "         [0.0008, 0.0011, 0.0004,  ..., 0.0065, 0.0008, 0.0048],\n",
       "         [0.0028, 0.0061, 0.0012,  ..., 0.0036, 0.0093, 0.0034]],\n",
       "\n",
       "        [[0.0029, 0.0058, 0.0018,  ..., 0.0011, 0.0011, 0.0004],\n",
       "         [0.0016, 0.0026, 0.0188,  ..., 0.0018, 0.0071, 0.0017],\n",
       "         [0.0203, 0.0027, 0.0011,  ..., 0.0067, 0.0008, 0.0029],\n",
       "         ...,\n",
       "         [0.0129, 0.0085, 0.0028,  ..., 0.0017, 0.0012, 0.0021],\n",
       "         [0.0018, 0.0020, 0.0039,  ..., 0.0008, 0.0094, 0.0024],\n",
       "         [0.0030, 0.0013, 0.0067,  ..., 0.0087, 0.0017, 0.0004]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0050, 0.0033, 0.0024,  ..., 0.0012, 0.0007, 0.0038],\n",
       "         [0.0031, 0.0037, 0.0104,  ..., 0.0006, 0.0016, 0.0017],\n",
       "         [0.0011, 0.0014, 0.0007,  ..., 0.0016, 0.0103, 0.0010],\n",
       "         ...,\n",
       "         [0.0121, 0.0031, 0.0061,  ..., 0.0066, 0.0020, 0.0003],\n",
       "         [0.0016, 0.0026, 0.0091,  ..., 0.0042, 0.0084, 0.0045],\n",
       "         [0.0035, 0.0213, 0.0025,  ..., 0.0013, 0.0044, 0.0037]],\n",
       "\n",
       "        [[0.0027, 0.0148, 0.0199,  ..., 0.0020, 0.0058, 0.0182],\n",
       "         [0.0013, 0.0004, 0.0019,  ..., 0.0058, 0.0009, 0.0069],\n",
       "         [0.0012, 0.0056, 0.0015,  ..., 0.0023, 0.0015, 0.0019],\n",
       "         ...,\n",
       "         [0.0039, 0.0028, 0.0031,  ..., 0.0033, 0.0012, 0.0020],\n",
       "         [0.0039, 0.0004, 0.0090,  ..., 0.0014, 0.0006, 0.0033],\n",
       "         [0.0023, 0.0008, 0.0003,  ..., 0.0071, 0.0015, 0.0009]],\n",
       "\n",
       "        [[0.0004, 0.0021, 0.0007,  ..., 0.0025, 0.0017, 0.0020],\n",
       "         [0.0070, 0.0056, 0.0008,  ..., 0.0009, 0.0025, 0.0005],\n",
       "         [0.0011, 0.0005, 0.0006,  ..., 0.0032, 0.0006, 0.0024],\n",
       "         ...,\n",
       "         [0.0071, 0.0045, 0.0023,  ..., 0.0015, 0.0007, 0.0017],\n",
       "         [0.0027, 0.0030, 0.0031,  ..., 0.0049, 0.0078, 0.0124],\n",
       "         [0.0024, 0.0021, 0.0014,  ..., 0.0025, 0.0023, 0.0023]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbase.layers[0].probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e467945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0116, 0.0033, 0.0048,  ..., 0.0011, 0.0020, 0.0005],\n",
       "         [0.0008, 0.0010, 0.0009,  ..., 0.0009, 0.0039, 0.0018],\n",
       "         [0.0026, 0.0023, 0.0004,  ..., 0.0013, 0.0220, 0.0153],\n",
       "         ...,\n",
       "         [0.0014, 0.0025, 0.0007,  ..., 0.0040, 0.0026, 0.0078],\n",
       "         [0.0026, 0.0010, 0.0092,  ..., 0.0038, 0.0009, 0.0006],\n",
       "         [0.0077, 0.0009, 0.0019,  ..., 0.0046, 0.0067, 0.0046]],\n",
       "\n",
       "        [[0.0085, 0.0005, 0.0011,  ..., 0.0014, 0.0112, 0.0040],\n",
       "         [0.0014, 0.0100, 0.0075,  ..., 0.0009, 0.0008, 0.0012],\n",
       "         [0.0021, 0.0035, 0.0036,  ..., 0.0021, 0.0023, 0.0007],\n",
       "         ...,\n",
       "         [0.0022, 0.0016, 0.0045,  ..., 0.0031, 0.0019, 0.0026],\n",
       "         [0.0008, 0.0011, 0.0004,  ..., 0.0065, 0.0008, 0.0048],\n",
       "         [0.0028, 0.0061, 0.0012,  ..., 0.0036, 0.0093, 0.0034]],\n",
       "\n",
       "        [[0.0029, 0.0058, 0.0018,  ..., 0.0011, 0.0011, 0.0004],\n",
       "         [0.0016, 0.0026, 0.0188,  ..., 0.0018, 0.0071, 0.0017],\n",
       "         [0.0203, 0.0027, 0.0011,  ..., 0.0067, 0.0008, 0.0029],\n",
       "         ...,\n",
       "         [0.0129, 0.0085, 0.0028,  ..., 0.0017, 0.0012, 0.0021],\n",
       "         [0.0018, 0.0020, 0.0039,  ..., 0.0008, 0.0094, 0.0024],\n",
       "         [0.0030, 0.0013, 0.0067,  ..., 0.0087, 0.0017, 0.0004]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0050, 0.0033, 0.0024,  ..., 0.0012, 0.0007, 0.0038],\n",
       "         [0.0031, 0.0037, 0.0104,  ..., 0.0006, 0.0016, 0.0017],\n",
       "         [0.0011, 0.0014, 0.0007,  ..., 0.0016, 0.0103, 0.0010],\n",
       "         ...,\n",
       "         [0.0121, 0.0031, 0.0061,  ..., 0.0066, 0.0020, 0.0003],\n",
       "         [0.0016, 0.0026, 0.0091,  ..., 0.0042, 0.0084, 0.0045],\n",
       "         [0.0035, 0.0213, 0.0025,  ..., 0.0013, 0.0044, 0.0037]],\n",
       "\n",
       "        [[0.0027, 0.0148, 0.0199,  ..., 0.0020, 0.0058, 0.0182],\n",
       "         [0.0013, 0.0004, 0.0019,  ..., 0.0058, 0.0009, 0.0069],\n",
       "         [0.0012, 0.0056, 0.0015,  ..., 0.0023, 0.0015, 0.0019],\n",
       "         ...,\n",
       "         [0.0039, 0.0028, 0.0031,  ..., 0.0033, 0.0012, 0.0020],\n",
       "         [0.0039, 0.0004, 0.0090,  ..., 0.0014, 0.0006, 0.0033],\n",
       "         [0.0023, 0.0008, 0.0003,  ..., 0.0071, 0.0015, 0.0009]],\n",
       "\n",
       "        [[0.0004, 0.0021, 0.0007,  ..., 0.0025, 0.0017, 0.0020],\n",
       "         [0.0070, 0.0056, 0.0008,  ..., 0.0009, 0.0025, 0.0005],\n",
       "         [0.0011, 0.0005, 0.0006,  ..., 0.0032, 0.0006, 0.0024],\n",
       "         ...,\n",
       "         [0.0071, 0.0045, 0.0023,  ..., 0.0015, 0.0007, 0.0017],\n",
       "         [0.0027, 0.0030, 0.0031,  ..., 0.0049, 0.0078, 0.0124],\n",
       "         [0.0024, 0.0021, 0.0014,  ..., 0.0025, 0.0023, 0.0023]]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpatch.layers[0].probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dce36c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0],\n",
       "        [1024],\n",
       "        [2048],\n",
       "        [   1],\n",
       "        [1025],\n",
       "        [2049],\n",
       "        [   2],\n",
       "        [1026],\n",
       "        [2050],\n",
       "        [   3],\n",
       "        [2051],\n",
       "        [1027],\n",
       "        [  32],\n",
       "        [1056],\n",
       "        [2080],\n",
       "        [  33]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbase.layers[0].scope_idx[0:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "182b3cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.1613e-09, 2.1431e-09, 3.6560e-09,  ..., 7.4792e-03,\n",
       "          5.0671e-08, 1.7350e-09],\n",
       "         [9.4486e-04, 7.6018e-09, 5.8930e-09,  ..., 2.0203e-03,\n",
       "          1.2374e-08, 1.0862e-08],\n",
       "         [2.6816e-09, 2.2701e-09, 3.5041e-09,  ..., 1.9916e-09,\n",
       "          2.4507e-09, 4.0534e-09],\n",
       "         ...,\n",
       "         [2.7938e-08, 6.6198e-08, 3.4376e-08,  ..., 1.7326e-08,\n",
       "          2.0657e-08, 1.3561e-07],\n",
       "         [5.4650e-09, 7.2708e-09, 7.1364e-09,  ..., 4.1515e-09,\n",
       "          5.6903e-09, 5.8939e-08],\n",
       "         [2.1024e-06, 1.4420e-06, 1.3987e-06,  ..., 2.0222e-07,\n",
       "          1.9726e-06, 1.8803e-06]],\n",
       "\n",
       "        [[8.9590e-07, 1.0745e-06, 7.6588e-07,  ..., 1.3676e-06,\n",
       "          2.2715e-06, 7.6267e-07],\n",
       "         [2.7846e-03, 4.7248e-09, 5.1307e-09,  ..., 5.1690e-09,\n",
       "          5.1418e-09, 7.1918e-09],\n",
       "         [2.2786e-08, 3.9076e-08, 1.0242e-07,  ..., 4.8132e-08,\n",
       "          4.9310e-08, 4.1642e-08],\n",
       "         ...,\n",
       "         [5.0377e-09, 4.8060e-09, 4.1626e-09,  ..., 4.4811e-06,\n",
       "          9.8012e-09, 4.2916e-09],\n",
       "         [1.0076e-05, 7.7120e-09, 5.2797e-09,  ..., 5.5294e-09,\n",
       "          5.5307e-09, 4.8407e-09],\n",
       "         [4.5389e-07, 4.3134e-08, 9.4122e-09,  ..., 1.1743e-04,\n",
       "          4.0283e-09, 5.0622e-09]],\n",
       "\n",
       "        [[4.9030e-08, 7.5887e-08, 5.9779e-08,  ..., 6.2942e-08,\n",
       "          3.9188e-08, 6.1575e-08],\n",
       "         [5.8880e-07, 3.5846e-06, 6.2215e-07,  ..., 2.3966e-05,\n",
       "          5.2189e-07, 4.8228e-06],\n",
       "         [4.5803e-09, 3.6081e-08, 4.7780e-09,  ..., 9.7322e-09,\n",
       "          7.2448e-09, 8.2970e-07],\n",
       "         ...,\n",
       "         [2.8339e-07, 3.1600e-07, 3.8934e-07,  ..., 1.9165e-06,\n",
       "          5.7834e-07, 1.3148e-06],\n",
       "         [3.7200e-07, 2.3339e-07, 4.5228e-07,  ..., 3.2287e-06,\n",
       "          3.2015e-07, 1.9903e-05],\n",
       "         [4.4217e-09, 4.4228e-09, 1.1654e-08,  ..., 1.9974e-08,\n",
       "          4.2173e-07, 3.4666e-08]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[2.8411e-09, 1.4543e-09, 3.4256e-09,  ..., 6.0285e-08,\n",
       "          1.0073e-08, 3.0902e-09],\n",
       "         [2.8289e-09, 5.9163e-10, 2.0712e-10,  ..., 3.7262e-10,\n",
       "          8.1146e-11, 2.8640e-09],\n",
       "         [1.1107e-08, 4.8149e-10, 8.6771e-10,  ..., 7.5689e-11,\n",
       "          1.2834e-10, 3.3440e-09],\n",
       "         ...,\n",
       "         [1.0671e-08, 5.6936e-09, 6.6789e-09,  ..., 6.1942e-09,\n",
       "          4.9645e-09, 6.3288e-09],\n",
       "         [5.0705e-05, 2.2571e-07, 8.0785e-07,  ..., 8.7163e-07,\n",
       "          3.1290e-07, 1.8396e-06],\n",
       "         [4.4416e-08, 3.4938e-08, 3.5077e-08,  ..., 3.2904e-08,\n",
       "          3.7311e-08, 5.3837e-08]],\n",
       "\n",
       "        [[1.6654e-07, 1.2767e-07, 9.6006e-08,  ..., 5.4587e-08,\n",
       "          4.6223e-07, 1.0063e-07],\n",
       "         [4.2257e-06, 5.7838e-06, 4.6190e-06,  ..., 4.0052e-06,\n",
       "          3.9801e-06, 3.1347e-04],\n",
       "         [1.9993e-09, 2.3778e-09, 1.2871e-08,  ..., 3.8520e-09,\n",
       "          3.4328e-09, 1.5622e-08],\n",
       "         ...,\n",
       "         [2.3048e-08, 2.1774e-08, 1.8858e-08,  ..., 2.1328e-08,\n",
       "          2.3340e-08, 4.4488e-03],\n",
       "         [1.1227e-08, 1.1006e-08, 1.0938e-08,  ..., 2.5486e-06,\n",
       "          1.5742e-08, 3.4354e-08],\n",
       "         [1.0893e-04, 4.9947e-09, 8.7130e-09,  ..., 4.7986e-09,\n",
       "          4.9469e-09, 1.3402e-08]],\n",
       "\n",
       "        [[1.6493e-09, 1.8704e-09, 1.4640e-09,  ..., 3.0569e-03,\n",
       "          1.1641e-05, 1.8501e-09],\n",
       "         [4.8941e-06, 4.9035e-06, 4.0602e-06,  ..., 4.9963e-06,\n",
       "          5.8404e-06, 5.0336e-06],\n",
       "         [1.2898e-10, 2.8985e-11, 2.6012e-07,  ..., 3.5045e-11,\n",
       "          2.5963e-10, 1.9011e-10],\n",
       "         ...,\n",
       "         [4.2254e-09, 4.5095e-09, 5.1795e-09,  ..., 4.4959e-03,\n",
       "          5.3437e-08, 4.3927e-09],\n",
       "         [1.0710e-05, 3.2144e-05, 8.6710e-07,  ..., 2.1579e-06,\n",
       "          5.1930e-05, 1.5234e-06],\n",
       "         [9.9512e-08, 1.1122e-07, 6.6776e-08,  ..., 1.0211e-04,\n",
       "          2.2131e-02, 9.9150e-08]]], device='cuda:2', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_circ_trained.layers[3].weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e99122",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86414589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d14cc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200: Average NLL: 11506.787\n",
      "Step 400: Average NLL: 11498.851\n",
      "Step 600: Average NLL: 11493.236\n",
      "Step 800: Average NLL: 11490.054\n",
      "Step 1000: Average NLL: 11482.754\n",
      "Step 1200: Average NLL: 11486.951\n",
      "Step 1400: Average NLL: 11485.080\n",
      "Step 1600: Average NLL: 11480.399\n",
      "Step 1800: Average NLL: 11479.311\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([16, 3072])\n",
      "torch.Size([16, 1, 1])\n",
      "Average test LL: 11538.918\n",
      "Bits per dimension: 5.419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'# trainable parameters': 88631298,\n",
       " 'train loss': [11506.787109375,\n",
       "  11498.8505859375,\n",
       "  11493.236328125,\n",
       "  11490.0537109375,\n",
       "  11482.75390625,\n",
       "  11486.951171875,\n",
       "  11485.080078125,\n",
       "  11480.3994140625,\n",
       "  11479.310546875],\n",
       " 'test loss': 11538.91751875,\n",
       " 'test bits per dimension': np.float64(5.418990586434703),\n",
       " 'train loss (min)': 11479.310546875,\n",
       " 'train time': 608.2784242630005,\n",
       " 'test time': 5.901854515075684}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cirkit.pipeline import compile\n",
    "import gc \n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def train_and_eval_circuit(circuit, patch: bool):\n",
    "    # Set some seeds\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    # torch.cuda.manual_seed(42)\n",
    "\n",
    "    # Set the torch device to use\n",
    "    device = torch.device(DEVICE)\n",
    "    # device=\"cpu\"\n",
    "    # Move the circuit to chosen device\n",
    "    circuit = circuit.to(device)\n",
    "\n",
    "    num_epochs = 10\n",
    "    step_idx = 0\n",
    "    running_loss = 0.0\n",
    "    running_samples = 0\n",
    "    stats = dict()\n",
    "\n",
    "    stats[\"# trainable parameters\"] = sum(\n",
    "        p.numel() for p in circuit.parameters() if p.requires_grad\n",
    "    )\n",
    "    stats[\"train loss\"] = []\n",
    "    patch_order=PatchOrderingLayer((1,*CIFAR_SIZE), KERNEL_SIZE)\n",
    "    # Initialize a torch optimizer of your choice,\n",
    "    #  e.g., Adam, by passing the parameters of the circuit\n",
    "    optimizer = torch.optim.Adam(circuit.parameters(), lr=0.05)\n",
    "    begin_train = time.time()\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        for i, (batch, _) in enumerate(train_dataloader):\n",
    "            # The circuit expects an input of shape (batch_dim, num_variables)\n",
    "            BS = batch.shape[0]\n",
    "            batch=batch.view(BS, -1)\n",
    "            if patch:\n",
    "                batch = patch_order(batch)\n",
    "            batch = batch.to(device)\n",
    "            # Compute the log-likelihoods of the batch, by evaluating the circuit\n",
    "            log_likelihoods = circuit(batch)\n",
    "\n",
    "            # We take the negated average log-likelihood as loss\n",
    "            loss = -torch.mean(log_likelihoods)\n",
    "            loss.backward()\n",
    "            # Update the parameters of the circuits, as any other model in PyTorch\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            running_loss += loss.detach() * len(batch)\n",
    "            running_samples += len(batch)\n",
    "            step_idx += 1\n",
    "            if step_idx % 200 == 0:\n",
    "                average_nll = running_loss / running_samples\n",
    "                print(f\"Step {step_idx}: Average NLL: {average_nll:.3f}\")\n",
    "                running_loss = 0.0\n",
    "                running_samples = 0\n",
    "\n",
    "                stats[\"train loss\"].append(average_nll.cpu().item())\n",
    "    end_train = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_lls = 0.0\n",
    "\n",
    "        for batch, _ in test_dataloader:\n",
    "            # The circuit expects an input of shape (batch_dim, num_variables)\n",
    "            BS = batch.shape[0]\n",
    "            batch=batch.view(BS, -1)\n",
    "            if patch:\n",
    "                batch = patch_order(batch)\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Compute the log-likelihoods of the batch\n",
    "            print(batch.shape)\n",
    "            log_likelihoods = circuit(batch)\n",
    "            print(log_likelihoods.shape)\n",
    "\n",
    "            # Accumulate the log-likelihoods\n",
    "            test_lls += log_likelihoods.sum().item()\n",
    "\n",
    "        # Compute average test log-likelihood and bits per dimension\n",
    "        average_nll = -test_lls / len(data_test)\n",
    "        bpd = average_nll / (32*32*3 * np.log(2.0))\n",
    "        print(f\"Average test LL: {average_nll:.3f}\")\n",
    "        print(f\"Bits per dimension: {bpd:.3f}\")\n",
    "\n",
    "        stats[\"test loss\"] = average_nll\n",
    "        stats[\"test bits per dimension\"] = bpd\n",
    "    end_test = time.time()\n",
    "\n",
    "    # Free GPU memory\n",
    "    circuit = circuit.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    stats[\"train loss (min)\"] = min(stats[\"train loss\"])\n",
    "    stats[\"train time\"] = end_train - begin_train\n",
    "    stats[\"test time\"] = end_test - end_train\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# results = dict()\n",
    "# for k, cc in circuits.items():\n",
    "#     print('\\nTraining circuit \"%s\"' % k)\n",
    "#     ctype = k.split(\"+\")[0].strip()\n",
    "#     results[k] = train_and_eval_circuit(cc, patch=False)\n",
    "#     results[k][\"type\"] = k.spli512t(\"+\")[0].strip()\n",
    "#     results[k][\"sum product layer\"] = k.split(\"+\")[1].strip()\n",
    "#     results[k][\"structure\"] = k.split(\"+\")[2].strip()\n",
    "# base_circ = cirkit_compile(stitched)\n",
    "# base_circ.reset_parameters()\n",
    "train_and_eval_circuit(cbase, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2324db16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3e2eda3610>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cirkit.backend.torch.queries import SamplingQuery\n",
    "from src.utils import unpatchify\n",
    "\n",
    "unpatch_fn = unpatchify(CIFAR_SIZE, KERNEL_SIZE, KERNEL_SIZE,1)\n",
    "circuit = cbase.cpu()\n",
    "query = SamplingQuery(circuit)\n",
    "\n",
    "samples, _ = query(num_samples=1)\n",
    "# samples = unpatch_fn(samples).reshape(28,28,1)\n",
    "plt.imshow(samples.reshape(3,32,32).permute(1,2,0), cmap=\"grey\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
