{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63aed413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from cirkit.templates import data_modalities, utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "PIXEL_RANGE = 255\n",
    "example_image = None\n",
    "\n",
    "KERNEL_SIZE = (1, 1)\n",
    "CIFAR_SIZE = (32, 32)\n",
    "DEVICE = \"cuda:5\"\n",
    "EPOCH = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9fdfac",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb33551",
   "metadata": {},
   "source": [
    "Let's define a function to create and use patches of the base Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ab5373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(kernel_size, stride, compile=True, contiguous_output=False):\n",
    "    kh, kw = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
    "    sh, sw = (stride, stride) if isinstance(stride, int) else stride\n",
    "\n",
    "    def _patchify(image: torch.Tensor):\n",
    "        # Accept (C,H,W) or (B,C,H,W)\n",
    "\n",
    "        # Ensure contiguous NCHW for predictable strides\n",
    "        x = image.contiguous()  # (B,C,H,W)\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Number of patches along H/W\n",
    "        Lh = (H - kh) // sh + 1\n",
    "        Lw = (W - kw) // sw + 1\n",
    "\n",
    "        # Create a zero-copy view: (B, C, Lh, Lw, kh, kw)\n",
    "        sN, sC, sH, sW = x.stride()\n",
    "        patches = x.as_strided(\n",
    "            size=(B, C, Lh, Lw, kh, kw),\n",
    "            stride=(sN, sC, sH * sh, sW * sw, sH, sW),\n",
    "        )\n",
    "        # Reorder to (B, P, C, kh, kw) where P = Lh*Lw\n",
    "        patches = patches.permute(0, 2, 3, 1, 4, 5).reshape(B * Lh * Lw, C, kh, kw)\n",
    "\n",
    "        if contiguous_output:\n",
    "            patches = (\n",
    "                patches.contiguous()\n",
    "            )  # materialize if the next ops need contiguous\n",
    "\n",
    "        return patches\n",
    "\n",
    "    if compile:\n",
    "        _patchify = torch.compile(_patchify, fullgraph=True, dynamic=False)\n",
    "    return _patchify\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: (PIXEL_RANGE * x).long()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_train = datasets.CIFAR10(\n",
    "    \"datasets\", train=True, download=True, transform=transform\n",
    ")\n",
    "data_test = datasets.CIFAR10(\n",
    "    \"datasets\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_idx, val_idx = train_test_split(range(len(data_train)), test_size=0.25)\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "# Instantiate the training and testing data loaders\n",
    "train_dataloader = DataLoader(data_train, batch_size=512, sampler=train_sampler)\n",
    "val_dataloader = DataLoader(data_train, batch_size=512, sampler=val_sampler)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa33a031",
   "metadata": {},
   "source": [
    "## Defining the Circuit\n",
    "\n",
    "We want to create a factory to create the different circuit we will want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b2a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.symbolic.circuit import Circuit, Scope\n",
    "from cirkit.symbolic.layers import (\n",
    "    GaussianLayer,\n",
    "    SumLayer,\n",
    "    HadamardLayer,\n",
    "    CategoricalLayer,\n",
    ")\n",
    "from cirkit.templates import utils\n",
    "\n",
    "\n",
    "def build_1_1() -> Circuit:\n",
    "    # This parametrizes the mixture weights such that they add up to one.\n",
    "    weight_factory = utils.parameterization_to_factory(\n",
    "        utils.Parameterization(\n",
    "            activation=\"softmax\",  # Parameterize the sum weights by using a softmax activation\n",
    "            initialization=\"uniform\",  # Initialize the sum weights by sampling from a standard normal distribution\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # We introduce one more mixture than in the original model\n",
    "    # Again, SGD/Adam is not the best way to fit a (shallow) Gaussian mixture model\n",
    "    units = 10\n",
    "\n",
    "    g0 = CategoricalLayer(Scope((0,)), units, num_categories=256)\n",
    "    g1 = CategoricalLayer(Scope((1,)), units, num_categories=256)\n",
    "    g2 = CategoricalLayer(Scope((2,)), units, num_categories=256)\n",
    "\n",
    "    s0 = SumLayer(units, 1, 1, weight_factory=weight_factory)\n",
    "    s2 = SumLayer(units, 1, 1, weight_factory=weight_factory)\n",
    "    s1 = SumLayer(units, 1, 1, weight_factory=weight_factory)\n",
    "\n",
    "    return Circuit(\n",
    "        layers=[\n",
    "            g0,\n",
    "            g1,\n",
    "            g2,\n",
    "            s0,\n",
    "            s1,\n",
    "            s2,\n",
    "        ],  # Layers that appear in the circuit (i.e. nodes in the graph)\n",
    "        in_layers={  # Connections between layers (i.e. edges in the graph as an adjacency list)\n",
    "            g0: [],\n",
    "            g1: [],\n",
    "            g2: [],\n",
    "            s0: [g0],\n",
    "            s1: [g1],\n",
    "            s2: [g2],\n",
    "        },\n",
    "        outputs=[s0, s1, s2],  # Nodes that are returned by the circuit\n",
    "    )\n",
    "\n",
    "\n",
    "def patch_circuit_factory(kernel_size, region_graph, layer_type, num_units):\n",
    "    return data_modalities.image_data(\n",
    "        (3, *kernel_size),\n",
    "        region_graph=region_graph,\n",
    "        input_layer=\"categorical\",\n",
    "        num_input_units=num_units,\n",
    "        sum_product_layer=layer_type,\n",
    "        num_sum_units=num_units,\n",
    "        sum_weight_param=utils.Parameterization(\n",
    "            activation=\"softmax\", initialization=\"normal\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def base_circuit_factory(region_graph, layer_type, num_units):\n",
    "    return data_modalities.image_data(\n",
    "        (3, *CIFAR_SIZE),\n",
    "        region_graph=region_graph,\n",
    "        input_layer=\"categorical\",\n",
    "        num_input_units=num_units,\n",
    "        sum_product_layer=layer_type,\n",
    "        num_sum_units=num_units,\n",
    "        sum_weight_param=utils.Parameterization(\n",
    "            activation=\"softmax\", initialization=\"normal\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def circuit_factory(circuit_type: str, **kwargs):\n",
    "    name = f\"{circuit_type}\"\n",
    "    for item in kwargs.values():\n",
    "        name += f\" + {item}\"\n",
    "    if circuit_type == \"patch\":\n",
    "        return name, patch_circuit_factory(KERNEL_SIZE, **kwargs)\n",
    "    else:\n",
    "        return name, base_circuit_factory(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa8a3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "explore_grid = {\n",
    "    \"circuit_type\": [\"patch\", \"base\"],\n",
    "    \"layer_type\": [\"cp-t\", \"cp\", \"tucker\"],\n",
    "    \"region_graph\": [\"quad-graph\", \"quad-tree-2\"],\n",
    "    \"num_units\": [16, 32, 64, 128],\n",
    "}\n",
    "keys, values = zip(*explore_grid.items())\n",
    "explore_list = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a57e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits = dict((circuit_factory(**config) for config in explore_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e99122",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d14cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from cirkit.pipeline import compile\n",
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "\n",
    "\n",
    "def get_flops(model, inp, with_backward=False):\n",
    "    istrain = model.training\n",
    "    model.eval()\n",
    "\n",
    "    inp = inp if isinstance(inp, torch.Tensor) else torch.randn(inp)\n",
    "\n",
    "    flop_counter = FlopCounterMode(mods=model, display=False, depth=None)\n",
    "    with flop_counter:\n",
    "        if with_backward:\n",
    "            model(inp).sum().backward()\n",
    "        else:\n",
    "            model(inp)\n",
    "    total_flops = flop_counter.get_total_flops()\n",
    "    if istrain:\n",
    "        model.train()\n",
    "    return total_flops\n",
    "\n",
    "\n",
    "def train_and_eval_circuit(cc, patch: bool):\n",
    "    torch.cuda.memory.reset_peak_memory_stats()\n",
    "    # Set some seeds\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    # torch.cuda.manual_seed(42)\n",
    "\n",
    "    # Set the torch device to use\n",
    "    device = torch.device(DEVICE)\n",
    "    # Move the circuit to chosen device\n",
    "    circuit = compile(cc)\n",
    "    circuit = circuit.to(device)\n",
    "\n",
    "    num_epochs = 20\n",
    "    step_idx = 0\n",
    "    running_loss = 0.0\n",
    "    running_samples = 0\n",
    "    stats = dict()\n",
    "\n",
    "    stats[\"# trainable parameters\"] = sum(\n",
    "        p.numel() for p in circuit.parameters() if p.requires_grad\n",
    "    )\n",
    "    stats[\"train loss\"] = []\n",
    "    patch_fn = patchify(KERNEL_SIZE, KERNEL_SIZE)\n",
    "    # Initialize a torch optimizer of your choice,\n",
    "    #  e.g., Adam, by passing the parameters of the circuit\n",
    "    optimizer = torch.optim.Adam(circuit.parameters(), lr=0.01)\n",
    "    begin_train = time.time()\n",
    "    keep_batch = None\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        for i, (batch, _) in enumerate(train_dataloader):\n",
    "            # The circuit expects an input of shape (batch_dim, num_variables)\n",
    "            if patch:\n",
    "                batch = patch_fn(batch)\n",
    "            BS = batch.shape[0]\n",
    "            batch = batch.view(BS, -1)\n",
    "            if keep_batch is None:\n",
    "                keep_batch = batch\n",
    "            batch = batch.to(device)\n",
    "            # Compute the log-likelihoods of the batch, by evaluating the circuit\n",
    "            log_likelihoods = circuit(batch)\n",
    "\n",
    "            # We take the negated average log-likelihood as loss\n",
    "            loss = -torch.mean(log_likelihoods)\n",
    "            loss.backward()\n",
    "            # Update the parameters of the circuits, as any other model in PyTorch\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            running_loss += loss.detach() * len(batch)\n",
    "            running_samples += len(batch)\n",
    "            step_idx += 1\n",
    "            if step_idx % 200 == 0:\n",
    "                average_nll = running_loss / running_samples\n",
    "                print(f\"Step {step_idx}: Average NLL: {average_nll:.3f}\")\n",
    "                running_loss = 0.0\n",
    "                running_samples = 0\n",
    "\n",
    "                stats[\"train loss\"].append(average_nll.cpu().item())\n",
    "    end_train = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_lls = 0.0\n",
    "\n",
    "        for batch, _ in val_dataloader:\n",
    "            # The circuit expects an input of shape (batch_dim, num_variables)\n",
    "            if patch:\n",
    "                batch = patch_fn(batch)\n",
    "            BS = batch.shape[0]\n",
    "            batch = batch.view(BS, -1).to(device)\n",
    "\n",
    "            # Compute the log-likelihoods of the batch\n",
    "            log_likelihoods = circuit(batch)\n",
    "\n",
    "            # Accumulate the log-likelihoods\n",
    "            test_lls += log_likelihoods.sum().item()\n",
    "\n",
    "        # Compute average test log-likelihood and bits per dimension\n",
    "        average_nll = -test_lls / len(data_test)\n",
    "        bpd = average_nll / (32 * 32 * 3 * np.log(2.0))\n",
    "        print(f\"Average test LL: {average_nll:.3f}\")\n",
    "        print(f\"Bits per dimension: {bpd:.3f}\")\n",
    "\n",
    "        stats[\"test loss\"] = average_nll\n",
    "        stats[\"test bits per dimension\"] = bpd\n",
    "    end_test = time.time()\n",
    "\n",
    "    stats[\"train loss (min)\"] = min(stats[\"train loss\"])\n",
    "    stats[\"train time\"] = end_train - begin_train\n",
    "    stats[\"test time\"] = end_test - end_train\n",
    "    stats[\"FLOPs\"] = get_flops(circuit, keep_batch.to(device))\n",
    "    stats[\"memory\"] = torch.cuda.memory.max_memory_allocated(device)\n",
    "    print(f\"Total Flops {stats['FLOPs']}\")\n",
    "    print(f\"Total Memory cost {stats['memory']}\")\n",
    "\n",
    "    # Free GPU memory\n",
    "    circuit = circuit.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# results = dict()\n",
    "# for k, cc in circuits.items():\n",
    "#     print('\\nTraining circuit \"%s\"' % k)\n",
    "#     ctype = k.split(\"+\")[0].strip()\n",
    "#     results[k] = train_and_eval_circuit(cc, patch=ctype == \"patch\")\n",
    "#     results[k][\"type\"] = k.split(\"+\")[0].strip()\n",
    "#     results[k][\"sum product layer\"] = k.split(\"+\")[1].strip()\n",
    "#     results[k][\"structure\"] = k.split(\"+\")[2].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7871276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200: Average NLL: 5.496\n",
      "Step 400: Average NLL: 5.477\n",
      "Step 600: Average NLL: 5.477\n",
      "Step 800: Average NLL: 5.478\n",
      "Step 1000: Average NLL: 5.477\n",
      "Step 1200: Average NLL: 5.478\n",
      "Step 1400: Average NLL: 5.477\n",
      "Average test LL: 21027.791\n",
      "Bits per dimension: 9.875\n",
      "Total Flops 31457280\n",
      "Total Memory cost 341210112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_821369/2720829373.py:15: UserWarning: mods argument is not needed anymore, you can stop passing it\n",
      "  flop_counter = FlopCounterMode(mods=model, display=False, depth=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'# trainable parameters': 7710,\n",
       " 'train loss': [5.496336460113525,\n",
       "  5.477449417114258,\n",
       "  5.477447509765625,\n",
       "  5.477670669555664,\n",
       "  5.4773030281066895,\n",
       "  5.477632999420166,\n",
       "  5.477457523345947],\n",
       " 'test loss': 21027.791075,\n",
       " 'test bits per dimension': np.float64(9.875224578369696),\n",
       " 'train loss (min)': 5.4773030281066895,\n",
       " 'train time': 120.21795463562012,\n",
       " 'test time': 1.78206205368042,\n",
       " 'FLOPs': 31457280,\n",
       " 'memory': 341210112}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circ = build_1_1()\n",
    "train_and_eval_circuit(circ, patch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c46e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.8599],\n",
       "         [-5.7600],\n",
       "         [-4.8966]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compile(circ)(torch.randint(0, 255, (1, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1046386d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finish Scanning model in </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.0177</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> seconds</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinish Scanning model in \u001b[0m\u001b[1;32m0.0177\u001b[0m\u001b[1;34m seconds\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-266.7532]],\n",
      "\n",
      "        [[-268.8200]],\n",
      "\n",
      "        [[-268.7206]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-267.1960]],\n",
      "\n",
      "        [[-268.8501]],\n",
      "\n",
      "        [[-268.8837]]], device='cuda:6', grad_fn=<TransposeBackward0>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m res \u001b[38;5;241m=\u001b[39m circuit(batch)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mcircuit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torchmeter/core.py:1429\u001b[0m, in \u001b[0;36mMeter.overview\u001b[0;34m(self, show_warning, *order)\u001b[0m\n\u001b[1;32m   1426\u001b[0m format_cell \u001b[38;5;241m=\u001b[39m partial(Panel, safe_box\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, expand\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, highlight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, box\u001b[38;5;241m=\u001b[39mHORIZONTALS)\n\u001b[1;32m   1428\u001b[0m container\u001b[38;5;241m.\u001b[39madd_renderable(format_cell(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_info, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[b]Model INFO[/]\u001b[39m\u001b[38;5;124m\"\u001b[39m, border_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morange1\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 1429\u001b[0m container\u001b[38;5;241m.\u001b[39mrenderables\u001b[38;5;241m.\u001b[39mextend([\n\u001b[1;32m   1430\u001b[0m     format_cell(\n\u001b[1;32m   1431\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstat_info(stat_name, show_warning\u001b[38;5;241m=\u001b[39mshow_warning),\n\u001b[1;32m   1432\u001b[0m         title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[b]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstat_name\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m INFO[/]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1433\u001b[0m         border_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcyan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1434\u001b[0m     )\n\u001b[1;32m   1435\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stat_name \u001b[38;5;129;01min\u001b[39;00m order\n\u001b[1;32m   1436\u001b[0m ])\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m container\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torchmeter/core.py:1431\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1426\u001b[0m format_cell \u001b[38;5;241m=\u001b[39m partial(Panel, safe_box\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, expand\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, highlight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, box\u001b[38;5;241m=\u001b[39mHORIZONTALS)\n\u001b[1;32m   1428\u001b[0m container\u001b[38;5;241m.\u001b[39madd_renderable(format_cell(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_info, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[b]Model INFO[/]\u001b[39m\u001b[38;5;124m\"\u001b[39m, border_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morange1\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1429\u001b[0m container\u001b[38;5;241m.\u001b[39mrenderables\u001b[38;5;241m.\u001b[39mextend([\n\u001b[1;32m   1430\u001b[0m     format_cell(\n\u001b[0;32m-> 1431\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstat_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_warning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_warning\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1432\u001b[0m         title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[b]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstat_name\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m INFO[/]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1433\u001b[0m         border_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcyan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1434\u001b[0m     )\n\u001b[1;32m   1435\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stat_name \u001b[38;5;129;01min\u001b[39;00m order\n\u001b[1;32m   1436\u001b[0m ])\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m container\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torchmeter/core.py:1313\u001b[0m, in \u001b[0;36mMeter.stat_info\u001b[0;34m(self, stat_or_statname, show_warning)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generates a formatted summary of the specified statistics.\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03mThis method provides a summary of the given statistics, including its name and the crucial data\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stat_or_statname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1313\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstat_or_statname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stat_or_statname, Statistics):\n\u001b[1;32m   1315\u001b[0m     stat \u001b[38;5;241m=\u001b[39m stat_or_statname\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torchmeter/core.py:993\u001b[0m, in \u001b[0;36mMeter.cal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# feed forwad\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ipt2device()\n\u001b[0;32m--> 993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mipt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mipt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;66;03m# remove hooks after measurement\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mremove() \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, hook_ls))\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/circuits.py:240\u001b[0m, in \u001b[0;36mTorchCircuit.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1881\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1880\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1883\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1885\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1886\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1829\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1826\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1827\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1829\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1832\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1833\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1834\u001b[0m     ):\n\u001b[1;32m   1835\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/circuits.py:261\u001b[0m, in \u001b[0;36mTorchCircuit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scope \u001b[38;5;129;01mand\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected some input \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, as the circuit has scope \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scope\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/circuits.py:273\u001b[0m, in \u001b[0;36mTorchCircuit._evaluate_layers\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_evaluate_layers\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# Evaluate layers on the given input\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (O, B, K)\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, O, K)\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;66;03m# If the circuit has empty scope, we squeeze the batch dimension, as it is 1\u001b[39;00m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/graph/modules.py:331\u001b[0m, in \u001b[0;36mTorchDiAcyclicGraph.evaluate\u001b[0;34m(self, x, module_fn)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     y \u001b[38;5;241m=\u001b[39m module_fn(module, \u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/layers/input.py:130\u001b[0m, in \u001b[0;36mTorchInputFunctionLayer.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1881\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1880\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1883\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1885\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1886\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1829\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1826\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1827\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1829\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1832\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1833\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1834\u001b[0m     ):\n\u001b[1;32m   1835\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/layers/input.py:277\u001b[0m, in \u001b[0;36mTorchExpFamilyLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 277\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_unnormalized_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msemiring\u001b[38;5;241m.\u001b[39mmap_from(x, LSESumSemiring)\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/layers/input.py:407\u001b[0m, in \u001b[0;36mTorchCategoricalLayer.log_unnormalized_likelihood\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m     logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    409\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits()\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/parameters/parameter.py:178\u001b[0m, in \u001b[0;36mTorchParameter.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1881\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1880\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1883\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1885\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1886\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1829\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1826\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1827\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1829\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1832\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1833\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1834\u001b[0m     ):\n\u001b[1;32m   1835\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/parameters/parameter.py:188\u001b[0m, in \u001b[0;36mTorchParameter.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate the parameter computational graph.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m            of each parameter tensor slice.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/graph/modules.py:331\u001b[0m, in \u001b[0;36mTorchDiAcyclicGraph.evaluate\u001b[0;34m(self, x, module_fn)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     y \u001b[38;5;241m=\u001b[39m module_fn(module, \u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/parameters/nodes.py:61\u001b[0m, in \u001b[0;36mTorchParameterInput.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1881\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1880\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1883\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1885\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1886\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1842\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1840\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1841\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1842\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1845\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torchmeter/statistic.py:564\u001b[0m, in \u001b[0;36mCalMeter.__not_support_hook\u001b[0;34m(self, module, ipt, opt)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_not_supported \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__stat_ls):\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__stat_ls\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetail_val_container(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    561\u001b[0m             Operation_Id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opnode\u001b[38;5;241m.\u001b[39mnode_id,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    562\u001b[0m             Operation_Name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opnode\u001b[38;5;241m.\u001b[39mname,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    563\u001b[0m             Operation_Type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opnode\u001b[38;5;241m.\u001b[39mtype,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 564\u001b[0m             Input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__iopt_repr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mipt\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    565\u001b[0m             Output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__iopt_repr(opt),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    566\u001b[0m         )\n\u001b[1;32m    567\u001b[0m     )\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torchmeter/statistic.py:352\u001b[0m, in \u001b[0;36mCalMeter.__iopt_repr\u001b[0;34m(self, iopt)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(iopt, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mset\u001b[39m)):\n\u001b[1;32m    351\u001b[0m     item_repr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__iopt_repr, iopt))\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(item_repr) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(item_repr) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mitem_repr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(iopt, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    355\u001b[0m     item_repr \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__iopt_repr(k), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__iopt_repr(v)) \n\u001b[1;32m    356\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m iopt\u001b[38;5;241m.\u001b[39mitems()]  \u001b[38;5;66;03m# fmt: skip\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# circuit =compile(list(circuits.values())[3])\n",
    "# circuit = Meter(circuit)\n",
    "# circuit.to(\"cuda:6\")\n",
    "batch, _ = next(iter(val_dataloader))\n",
    "patch_fn = patchify(KERNEL_SIZE, KERNEL_SIZE)\n",
    "\n",
    "batch = patch_fn(batch)\n",
    "BS = batch.shape[0]\n",
    "batch = batch.view(BS, -1).to(\"cuda:6\")\n",
    "\n",
    "res = circuit(batch)\n",
    "print(res)\n",
    "circuit.overview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d85ddeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># trainable parameters</th>\n",
       "      <th>test loss</th>\n",
       "      <th>test bits per dimension</th>\n",
       "      <th>train loss (min)</th>\n",
       "      <th>train time</th>\n",
       "      <th>test time</th>\n",
       "      <th>type</th>\n",
       "      <th>sum product layer</th>\n",
       "      <th>structure</th>\n",
       "      <th>train time format</th>\n",
       "      <th>test time format</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>patch</th>\n",
       "      <td>8,135,170</td>\n",
       "      <td>12,483.816</td>\n",
       "      <td>5.863</td>\n",
       "      <td>194.937</td>\n",
       "      <td>1,048.771</td>\n",
       "      <td>3.044</td>\n",
       "      <td>patch</td>\n",
       "      <td>tucker</td>\n",
       "      <td>quad-graph</td>\n",
       "      <td>01:28</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patch</th>\n",
       "      <td>4,460,544</td>\n",
       "      <td>12,647.629</td>\n",
       "      <td>5.940</td>\n",
       "      <td>197.561</td>\n",
       "      <td>535.622</td>\n",
       "      <td>2.226</td>\n",
       "      <td>patch</td>\n",
       "      <td>tucker</td>\n",
       "      <td>quad-tree-2</td>\n",
       "      <td>01:55</td>\n",
       "      <td>01:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patch</th>\n",
       "      <td>1,032,834</td>\n",
       "      <td>12,685.412</td>\n",
       "      <td>5.957</td>\n",
       "      <td>198.096</td>\n",
       "      <td>294.293</td>\n",
       "      <td>1.839</td>\n",
       "      <td>patch</td>\n",
       "      <td>cp</td>\n",
       "      <td>quad-graph</td>\n",
       "      <td>01:54</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patch</th>\n",
       "      <td>901,762</td>\n",
       "      <td>12,689.151</td>\n",
       "      <td>5.959</td>\n",
       "      <td>198.086</td>\n",
       "      <td>286.904</td>\n",
       "      <td>1.766</td>\n",
       "      <td>patch</td>\n",
       "      <td>cp.T</td>\n",
       "      <td>quad-graph</td>\n",
       "      <td>01:46</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patch</th>\n",
       "      <td>909,376</td>\n",
       "      <td>12,832.732</td>\n",
       "      <td>6.027</td>\n",
       "      <td>200.429</td>\n",
       "      <td>268.187</td>\n",
       "      <td>1.509</td>\n",
       "      <td>patch</td>\n",
       "      <td>cp</td>\n",
       "      <td>quad-tree-2</td>\n",
       "      <td>01:28</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patch</th>\n",
       "      <td>843,840</td>\n",
       "      <td>12,860.927</td>\n",
       "      <td>6.040</td>\n",
       "      <td>200.870</td>\n",
       "      <td>261.979</td>\n",
       "      <td>1.584</td>\n",
       "      <td>patch</td>\n",
       "      <td>cp.T</td>\n",
       "      <td>quad-tree-2</td>\n",
       "      <td>01:21</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>67,136,130</td>\n",
       "      <td>13,412.097</td>\n",
       "      <td>6.299</td>\n",
       "      <td>13,043.954</td>\n",
       "      <td>364.976</td>\n",
       "      <td>1.746</td>\n",
       "      <td>base</td>\n",
       "      <td>cp</td>\n",
       "      <td>quad-graph</td>\n",
       "      <td>01:04</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>586,205,698</td>\n",
       "      <td>13,430.028</td>\n",
       "      <td>6.307</td>\n",
       "      <td>13,018.108</td>\n",
       "      <td>1,864.905</td>\n",
       "      <td>3.985</td>\n",
       "      <td>base</td>\n",
       "      <td>tucker</td>\n",
       "      <td>quad-graph</td>\n",
       "      <td>01:04</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>58,712,128</td>\n",
       "      <td>13,458.212</td>\n",
       "      <td>6.320</td>\n",
       "      <td>13,103.154</td>\n",
       "      <td>300.240</td>\n",
       "      <td>1.567</td>\n",
       "      <td>base</td>\n",
       "      <td>cp</td>\n",
       "      <td>quad-tree-2</td>\n",
       "      <td>01:00</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>318,246,912</td>\n",
       "      <td>13,459.455</td>\n",
       "      <td>6.321</td>\n",
       "      <td>13,045.494</td>\n",
       "      <td>992.584</td>\n",
       "      <td>2.709</td>\n",
       "      <td>base</td>\n",
       "      <td>tucker</td>\n",
       "      <td>quad-tree-2</td>\n",
       "      <td>01:32</td>\n",
       "      <td>01:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>58,747,522</td>\n",
       "      <td>13,515.246</td>\n",
       "      <td>6.347</td>\n",
       "      <td>13,064.032</td>\n",
       "      <td>336.427</td>\n",
       "      <td>1.624</td>\n",
       "      <td>base</td>\n",
       "      <td>cp.T</td>\n",
       "      <td>quad-graph</td>\n",
       "      <td>01:36</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>54,517,824</td>\n",
       "      <td>13,755.585</td>\n",
       "      <td>6.460</td>\n",
       "      <td>13,258.001</td>\n",
       "      <td>273.429</td>\n",
       "      <td>1.506</td>\n",
       "      <td>base</td>\n",
       "      <td>cp.T</td>\n",
       "      <td>quad-tree-2</td>\n",
       "      <td>01:33</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       # trainable parameters  test loss  test bits per dimension  \\\n",
       "patch               8,135,170 12,483.816                    5.863   \n",
       "patch               4,460,544 12,647.629                    5.940   \n",
       "patch               1,032,834 12,685.412                    5.957   \n",
       "patch                 901,762 12,689.151                    5.959   \n",
       "patch                 909,376 12,832.732                    6.027   \n",
       "patch                 843,840 12,860.927                    6.040   \n",
       "base               67,136,130 13,412.097                    6.299   \n",
       "base              586,205,698 13,430.028                    6.307   \n",
       "base               58,712,128 13,458.212                    6.320   \n",
       "base              318,246,912 13,459.455                    6.321   \n",
       "base               58,747,522 13,515.246                    6.347   \n",
       "base               54,517,824 13,755.585                    6.460   \n",
       "\n",
       "        train loss (min)  train time  test time   type sum product layer  \\\n",
       "patch            194.937   1,048.771      3.044  patch            tucker   \n",
       "patch            197.561     535.622      2.226  patch            tucker   \n",
       "patch            198.096     294.293      1.839  patch                cp   \n",
       "patch            198.086     286.904      1.766  patch              cp.T   \n",
       "patch            200.429     268.187      1.509  patch                cp   \n",
       "patch            200.870     261.979      1.584  patch              cp.T   \n",
       "base          13,043.954     364.976      1.746   base                cp   \n",
       "base          13,018.108   1,864.905      3.985   base            tucker   \n",
       "base          13,103.154     300.240      1.567   base                cp   \n",
       "base          13,045.494     992.584      2.709   base            tucker   \n",
       "base          13,064.032     336.427      1.624   base              cp.T   \n",
       "base          13,258.001     273.429      1.506   base              cp.T   \n",
       "\n",
       "          structure train time format test time format  \n",
       "patch    quad-graph             01:28            01:03  \n",
       "patch   quad-tree-2             01:55            01:02  \n",
       "patch    quad-graph             01:54            01:01  \n",
       "patch    quad-graph             01:46            01:01  \n",
       "patch   quad-tree-2             01:28            01:01  \n",
       "patch   quad-tree-2             01:21            01:01  \n",
       "base     quad-graph             01:04            01:01  \n",
       "base     quad-graph             01:04            01:03  \n",
       "base    quad-tree-2             01:00            01:01  \n",
       "base    quad-tree-2             01:32            01:02  \n",
       "base     quad-graph             01:36            01:01  \n",
       "base    quad-tree-2             01:33            01:01  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "df = df.drop(columns=\"train loss\")\n",
    "\n",
    "df.index = df.index.map(lambda x: x.split(\"+\")[0])\n",
    "df[\"# trainable parameters\"] = df[\"# trainable parameters\"].map(\"{:,d}\".format)\n",
    "pd.options.display.float_format = \"{:,.3f}\".format\n",
    "df[\"train time format\"] = pd.to_datetime(df[\"train time\"], unit=\"s\").dt.strftime(\n",
    "    \"%m:%S\"\n",
    ")\n",
    "df[\"test time format\"] = pd.to_datetime(df[\"test time\"], unit=\"s\").dt.strftime(\"%m:%S\")\n",
    "\n",
    "df.sort_values(\"test bits per dimension\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d635c345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllrrll}\n",
      "\\toprule\n",
      "type & sum product layer & structure & \\# trainable parameters & test bits per dimension & test loss & train time format & test time format \\\\\n",
      "\\midrule\n",
      "patch & tucker & quad-graph & 8,135,170 & 5.86 & 12483.82 & 01:28 & 01:03 \\\\\n",
      "patch & tucker & quad-tree-2 & 4,460,544 & 5.94 & 12647.63 & 01:55 & 01:02 \\\\\n",
      "patch & cp & quad-graph & 1,032,834 & 5.96 & 12685.41 & 01:54 & 01:01 \\\\\n",
      "patch & cp.T & quad-graph & 901,762 & 5.96 & 12689.15 & 01:46 & 01:01 \\\\\n",
      "patch & cp & quad-tree-2 & 909,376 & 6.03 & 12832.73 & 01:28 & 01:01 \\\\\n",
      "patch & cp.T & quad-tree-2 & 843,840 & 6.04 & 12860.93 & 01:21 & 01:01 \\\\\n",
      "base & cp & quad-graph & 67,136,130 & 6.30 & 13412.10 & 01:04 & 01:01 \\\\\n",
      "base & tucker & quad-graph & 586,205,698 & 6.31 & 13430.03 & 01:04 & 01:03 \\\\\n",
      "base & cp & quad-tree-2 & 58,712,128 & 6.32 & 13458.21 & 01:00 & 01:01 \\\\\n",
      "base & tucker & quad-tree-2 & 318,246,912 & 6.32 & 13459.45 & 01:32 & 01:02 \\\\\n",
      "base & cp.T & quad-graph & 58,747,522 & 6.35 & 13515.25 & 01:36 & 01:01 \\\\\n",
      "base & cp.T & quad-tree-2 & 54,517,824 & 6.46 & 13755.58 & 01:33 & 01:01 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    df[\n",
    "        [\n",
    "            \"type\",\n",
    "            \"sum product layer\",\n",
    "            \"structure\",\n",
    "            \"# trainable parameters\",\n",
    "            \"test bits per dimension\",\n",
    "            \"test loss\",\n",
    "            \"train time format\",\n",
    "            \"test time format\",\n",
    "        ]\n",
    "    ]\n",
    "    .sort_values(\"test bits per dimension\")\n",
    "    .to_latex(float_format=\"%.2f\", escape=True, index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b246223",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"bench_cifar.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
