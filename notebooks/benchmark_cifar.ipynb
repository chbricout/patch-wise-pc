{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63aed413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from cirkit.templates import data_modalities, utils\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "PIXEL_RANGE=255\n",
    "example_image = None\n",
    "\n",
    "KERNEL_SIZE=(4,4)\n",
    "CIFAR_SIZE=(32,32)\n",
    "DEVICE=\"cuda:5\"\n",
    "EPOCH=30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9fdfac",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb33551",
   "metadata": {},
   "source": [
    "Let's define a function to create and use patches of the base Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14ab5373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(kernel_size, stride,compile=True, contiguous_output=False):\n",
    "    kh, kw = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
    "    sh, sw = (stride, stride) if isinstance(stride, int) else stride\n",
    "    def _patchify(image: torch.Tensor):\n",
    "        # Accept (C,H,W) or (B,C,H,W)\n",
    "\n",
    "        # Ensure contiguous NCHW for predictable strides\n",
    "        x = image.contiguous()  # (B,C,H,W)\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Number of patches along H/W\n",
    "        Lh = (H - kh) // sh + 1\n",
    "        Lw = (W - kw) // sw + 1\n",
    "\n",
    "        # Create a zero-copy view: (B, C, Lh, Lw, kh, kw)\n",
    "        sN, sC, sH, sW = x.stride()\n",
    "        patches = x.as_strided(\n",
    "            size=(B, C, Lh, Lw, kh, kw),\n",
    "            stride=(sN, sC, sH * sh, sW * sw, sH, sW),\n",
    "        )\n",
    "        # Reorder to (B, P, C, kh, kw) where P = Lh*Lw\n",
    "        patches = patches.permute(0, 2, 3, 1, 4, 5).reshape(B * Lh * Lw, C, kh, kw)\n",
    "\n",
    "        if contiguous_output:\n",
    "            patches = patches.contiguous()  # materialize if the next ops need contiguous\n",
    "\n",
    "        return patches\n",
    "    if compile:\n",
    "        _patchify = torch.compile(_patchify, fullgraph=True, dynamic=False)\n",
    "    return _patchify\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: (PIXEL_RANGE * x).long()),\n",
    "])\n",
    "\n",
    "data_train = datasets.CIFAR10('datasets', train=True, download=True, transform=transform)\n",
    "data_test = datasets.CIFAR10('datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "train_idx, val_idx = train_test_split(range(len(data_train)), test_size=0.25)\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "# Instantiate the training and testing data loaders\n",
    "train_dataloader = DataLoader(data_train,  batch_size=128, sampler=train_sampler)\n",
    "val_dataloader = DataLoader(data_train,  batch_size=128, sampler=val_sampler)\n",
    "test_dataloader= DataLoader(data_test, shuffle=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa33a031",
   "metadata": {},
   "source": [
    "## Defining the Circuit\n",
    "\n",
    "We want to create a factory to create the different circuit we will want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1b2a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_circuit_factory(kernel_size, region_graph, layer_type, num_units):\n",
    "    return data_modalities.image_data(\n",
    "        (3,*kernel_size),\n",
    "        region_graph=region_graph,\n",
    "        input_layer=\"categorical\",\n",
    "        num_input_units=num_units,\n",
    "        sum_product_layer=layer_type,\n",
    "        num_sum_units=num_units,\n",
    "        sum_weight_param=utils.Parameterization(\n",
    "                activation='softmax',   \n",
    "                initialization='normal' \n",
    "            )\n",
    "    )\n",
    "\n",
    "def base_circuit_factory(region_graph, layer_type, num_units):\n",
    "    return data_modalities.image_data(\n",
    "        (3,*CIFAR_SIZE),\n",
    "        region_graph=region_graph,\n",
    "        input_layer=\"categorical\",\n",
    "        num_input_units=num_units,\n",
    "        sum_product_layer=layer_type,\n",
    "        num_sum_units=num_units,\n",
    "        sum_weight_param=utils.Parameterization(\n",
    "                activation='softmax',   \n",
    "                initialization='normal' \n",
    "            )\n",
    "    )\n",
    "def circuit_factory(circuit_type:str, **kwargs):\n",
    "    name =f\"{circuit_type}\"\n",
    "    for item in kwargs.values():\n",
    "        name+=f\" + {item}\"\n",
    "    if circuit_type==\"patch\":\n",
    "        return name, patch_circuit_factory(KERNEL_SIZE, **kwargs)\n",
    "    else:\n",
    "        return name, base_circuit_factory(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa8a3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "explore_grid = {\n",
    "    \"circuit_type\":[\"patch\", \"base\"],\n",
    "    \"layer_type\":[\"cp-t\", \"cp\", \"tucker\"],\n",
    "    \"region_graph\":[\"quad-graph\", \"quad-tree-2\"],\n",
    "    \"num_units\":[16,32,64,128],\n",
    "}\n",
    "keys, values = zip(*explore_grid.items())\n",
    "explore_list = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a57e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits=dict((circuit_factory(**config) for config in explore_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e99122",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d14cc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training circuit \"patch + cp-t + quad-graph + 16\"\n",
      "Step 200: Average NLL: 252.240\n",
      "Step 400: Average NLL: 223.000\n",
      "Step 600: Average NLL: 217.718\n",
      "Step 800: Average NLL: 215.115\n",
      "Step 1000: Average NLL: 214.241\n",
      "Step 1200: Average NLL: 213.539\n",
      "Step 1400: Average NLL: 213.242\n",
      "Average test LL: 17029.875\n",
      "Bits per dimension: 7.998\n",
      "Total Flops 151552000\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + cp-t + quad-graph + 32\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_312109/3724237691.py:16: UserWarning: mods argument is not needed anymore, you can stop passing it\n",
      "  flop_counter = FlopCounterMode(mods=model, display=False, depth=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200: Average NLL: 250.062\n",
      "Step 400: Average NLL: 219.549\n",
      "Step 600: Average NLL: 213.668\n",
      "Step 800: Average NLL: 210.849\n",
      "Step 1000: Average NLL: 209.012\n",
      "Step 1200: Average NLL: 208.117\n",
      "Step 1400: Average NLL: 207.331\n",
      "Average test LL: 16544.741\n",
      "Bits per dimension: 7.770\n",
      "Total Flops 605061120\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + cp-t + quad-graph + 64\"\n",
      "Step 200: Average NLL: 248.428\n",
      "Step 400: Average NLL: 218.175\n",
      "Step 600: Average NLL: 211.471\n",
      "Step 800: Average NLL: 207.692\n",
      "Step 1000: Average NLL: 205.802\n",
      "Step 1200: Average NLL: 204.266\n",
      "Step 1400: Average NLL: 203.451\n",
      "Average test LL: 16232.246\n",
      "Bits per dimension: 7.623\n",
      "Total Flops 2418049024\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + cp-t + quad-graph + 128\"\n",
      "Step 200: Average NLL: 247.183\n",
      "Step 400: Average NLL: 216.095\n",
      "Step 600: Average NLL: 208.370\n",
      "Step 800: Average NLL: 204.541\n",
      "Step 1000: Average NLL: 202.672\n",
      "Step 1200: Average NLL: 201.323\n",
      "Step 1400: Average NLL: 200.407\n",
      "Average test LL: 15973.672\n",
      "Bits per dimension: 7.502\n",
      "Total Flops 9667903488\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + cp-t + quad-tree-2 + 16\"\n",
      "Step 200: Average NLL: 252.287\n",
      "Step 400: Average NLL: 224.653\n",
      "Step 600: Average NLL: 219.828\n",
      "Step 800: Average NLL: 217.459\n",
      "Step 1000: Average NLL: 216.367\n",
      "Step 1200: Average NLL: 215.655\n",
      "Step 1400: Average NLL: 215.223\n",
      "Average test LL: 17182.578\n",
      "Bits per dimension: 8.069\n",
      "Total Flops 58982400\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + cp-t + quad-tree-2 + 32\"\n",
      "Step 200: Average NLL: 250.108\n",
      "Step 400: Average NLL: 221.841\n",
      "Step 600: Average NLL: 216.381\n",
      "Step 800: Average NLL: 213.347\n",
      "Step 1000: Average NLL: 211.687\n",
      "Step 1200: Average NLL: 210.220\n",
      "Step 1400: Average NLL: 209.552\n",
      "Average test LL: 16723.204\n",
      "Bits per dimension: 7.854\n",
      "Total Flops 235405312\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + cp-t + quad-tree-2 + 64\"\n",
      "Step 200: Average NLL: 248.490\n",
      "Step 400: Average NLL: 219.929\n",
      "Step 600: Average NLL: 213.453\n",
      "Step 800: Average NLL: 209.468\n",
      "Step 1000: Average NLL: 207.308\n",
      "Step 1200: Average NLL: 206.170\n",
      "Step 1400: Average NLL: 205.462\n",
      "Average test LL: 16397.764\n",
      "Bits per dimension: 7.701\n",
      "Total Flops 940572672\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + cp-t + quad-tree-2 + 128\"\n",
      "Step 200: Average NLL: 247.590\n",
      "Step 400: Average NLL: 218.491\n",
      "Step 600: Average NLL: 211.459\n",
      "Step 800: Average NLL: 207.519\n",
      "Step 1000: Average NLL: 205.208\n",
      "Step 1200: Average NLL: 203.992\n",
      "Step 1400: Average NLL: 203.234\n",
      "Average test LL: 16205.888\n",
      "Bits per dimension: 7.611\n",
      "Total Flops 3760193536\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + cp + quad-graph + 16\"\n",
      "Step 200: Average NLL: 253.936\n",
      "Step 400: Average NLL: 224.329\n",
      "Step 600: Average NLL: 218.579\n",
      "Step 800: Average NLL: 216.318\n",
      "Step 1000: Average NLL: 214.693\n",
      "Step 1200: Average NLL: 213.766\n",
      "Step 1400: Average NLL: 213.173\n",
      "Average test LL: 17017.843\n",
      "Bits per dimension: 7.992\n",
      "Total Flops 285769728\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + cp + quad-graph + 32\"\n",
      "Step 200: Average NLL: 252.163\n",
      "Step 400: Average NLL: 220.592\n",
      "Step 600: Average NLL: 214.802\n",
      "Step 800: Average NLL: 212.344\n",
      "Step 1000: Average NLL: 210.430\n",
      "Step 1200: Average NLL: 209.413\n",
      "Step 1400: Average NLL: 208.523\n",
      "Average test LL: 16630.650\n",
      "Bits per dimension: 7.810\n",
      "Total Flops 1141932032\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + cp + quad-graph + 64\"\n",
      "Step 200: Average NLL: 250.421\n",
      "Step 400: Average NLL: 218.659\n",
      "Step 600: Average NLL: 212.524\n",
      "Step 800: Average NLL: 209.141\n",
      "Step 1000: Average NLL: 207.407\n",
      "Step 1200: Average NLL: 205.711\n",
      "Step 1400: Average NLL: 204.641\n",
      "Average test LL: 16284.335\n",
      "Bits per dimension: 7.648\n",
      "Total Flops 4565532672\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + cp + quad-graph + 128\"\n",
      "Step 200: Average NLL: 249.998\n",
      "Step 400: Average NLL: 217.332\n",
      "Step 600: Average NLL: 209.641\n",
      "Step 800: Average NLL: 205.521\n",
      "Step 1000: Average NLL: 203.196\n",
      "Step 1200: Average NLL: 201.610\n",
      "Step 1400: Average NLL: 200.342\n",
      "Average test LL: 15949.654\n",
      "Bits per dimension: 7.490\n",
      "Total Flops 18257838080\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + cp + quad-tree-2 + 16\"\n",
      "Step 200: Average NLL: 254.933\n",
      "Step 400: Average NLL: 225.039\n",
      "Step 600: Average NLL: 219.573\n",
      "Step 800: Average NLL: 217.635\n",
      "Step 1000: Average NLL: 216.964\n",
      "Step 1200: Average NLL: 215.749\n",
      "Step 1400: Average NLL: 214.677\n",
      "Average test LL: 17123.381\n",
      "Bits per dimension: 8.042\n",
      "Total Flops 126091264\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + cp + quad-tree-2 + 32\"\n",
      "Step 200: Average NLL: 252.532\n",
      "Step 400: Average NLL: 221.275\n",
      "Step 600: Average NLL: 215.805\n",
      "Step 800: Average NLL: 213.247\n",
      "Step 1000: Average NLL: 211.590\n",
      "Step 1200: Average NLL: 210.612\n",
      "Step 1400: Average NLL: 209.931\n",
      "Average test LL: 16760.585\n",
      "Bits per dimension: 7.871\n",
      "Total Flops 503840768\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + cp + quad-tree-2 + 64\"\n",
      "Step 200: Average NLL: 250.942\n",
      "Step 400: Average NLL: 219.718\n",
      "Step 600: Average NLL: 213.608\n",
      "Step 800: Average NLL: 210.629\n",
      "Step 1000: Average NLL: 208.583\n",
      "Step 1200: Average NLL: 207.124\n",
      "Step 1400: Average NLL: 206.037\n",
      "Average test LL: 16397.804\n",
      "Bits per dimension: 7.701\n",
      "Total Flops 2014314496\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + cp + quad-tree-2 + 128\"\n",
      "Step 200: Average NLL: 250.224\n",
      "Step 400: Average NLL: 218.172\n",
      "Step 600: Average NLL: 211.050\n",
      "Step 800: Average NLL: 207.783\n",
      "Step 1000: Average NLL: 206.200\n",
      "Step 1200: Average NLL: 204.704\n",
      "Step 1400: Average NLL: 203.779\n",
      "Average test LL: 16229.625\n",
      "Bits per dimension: 7.622\n",
      "Total Flops 8055160832\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + tucker + quad-graph + 16\"\n",
      "Step 200: Average NLL: 255.053\n",
      "Step 400: Average NLL: 225.305\n",
      "Step 600: Average NLL: 218.026\n",
      "Step 800: Average NLL: 215.132\n",
      "Step 1000: Average NLL: 213.469\n",
      "Step 1200: Average NLL: 212.328\n",
      "Step 1400: Average NLL: 211.974\n",
      "Average test LL: 16916.521\n",
      "Bits per dimension: 7.944\n",
      "Total Flops 1921548288\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + tucker + quad-graph + 32\"\n",
      "Step 200: Average NLL: 253.010\n",
      "Step 400: Average NLL: 222.845\n",
      "Step 600: Average NLL: 214.083\n",
      "Step 800: Average NLL: 210.360\n",
      "Step 1000: Average NLL: 208.506\n",
      "Step 1200: Average NLL: 207.398\n",
      "Step 1400: Average NLL: 206.502\n",
      "Average test LL: 16456.634\n",
      "Bits per dimension: 7.728\n",
      "Total Flops 15201239040\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + tucker + quad-graph + 64\"\n",
      "Step 200: Average NLL: 251.529\n",
      "Step 400: Average NLL: 222.068\n",
      "Step 600: Average NLL: 211.807\n",
      "Step 800: Average NLL: 206.908\n",
      "Step 1000: Average NLL: 204.334\n",
      "Step 1200: Average NLL: 202.672\n",
      "Step 1400: Average NLL: 201.643\n",
      "Average test LL: 16061.938\n",
      "Bits per dimension: 7.543\n",
      "Total Flops 120932302848\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + tucker + quad-graph + 128\"\n",
      "Step 200: Average NLL: 251.059\n",
      "Step 400: Average NLL: 222.517\n",
      "Step 600: Average NLL: 209.869\n",
      "Step 800: Average NLL: 204.041\n",
      "Step 1000: Average NLL: 201.007\n",
      "Step 1200: Average NLL: 199.062\n",
      "Step 1400: Average NLL: 197.933\n",
      "Average test LL: 15751.291\n",
      "Bits per dimension: 7.397\n",
      "Total Flops 964761255936\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + tucker + quad-tree-2 + 16\"\n",
      "Step 200: Average NLL: 255.175\n",
      "Step 400: Average NLL: 226.218\n",
      "Step 600: Average NLL: 219.675\n",
      "Step 800: Average NLL: 216.910\n",
      "Step 1000: Average NLL: 215.482\n",
      "Step 1200: Average NLL: 214.529\n",
      "Step 1400: Average NLL: 213.926\n",
      "Average test LL: 17068.728\n",
      "Bits per dimension: 8.016\n",
      "Total Flops 943980544\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + tucker + quad-tree-2 + 32\"\n",
      "Step 200: Average NLL: 252.956\n",
      "Step 400: Average NLL: 223.820\n",
      "Step 600: Average NLL: 215.904\n",
      "Step 800: Average NLL: 212.378\n",
      "Step 1000: Average NLL: 210.642\n",
      "Step 1200: Average NLL: 209.146\n",
      "Step 1400: Average NLL: 208.340\n",
      "Average test LL: 16596.295\n",
      "Bits per dimension: 7.794\n",
      "Total Flops 7533494272\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + tucker + quad-tree-2 + 64\"\n",
      "Step 200: Average NLL: 251.691\n",
      "Step 400: Average NLL: 223.134\n",
      "Step 600: Average NLL: 213.799\n",
      "Step 800: Average NLL: 209.894\n",
      "Step 1000: Average NLL: 207.382\n",
      "Step 1200: Average NLL: 205.669\n",
      "Step 1400: Average NLL: 204.075\n",
      "Average test LL: 16256.827\n",
      "Bits per dimension: 7.635\n",
      "Total Flops 60197699584\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"patch + tucker + quad-tree-2 + 128\"\n",
      "Step 200: Average NLL: 251.103\n",
      "Step 400: Average NLL: 223.372\n",
      "Step 600: Average NLL: 211.793\n",
      "Step 800: Average NLL: 205.945\n",
      "Step 1000: Average NLL: 203.285\n",
      "Step 1200: Average NLL: 201.385\n",
      "Step 1400: Average NLL: 200.509\n",
      "Average test LL: 15975.784\n",
      "Bits per dimension: 7.503\n",
      "Total Flops 481306869760\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + cp-t + quad-graph + 16\"\n",
      "Step 200: Average NLL: 17058.082\n",
      "Step 400: Average NLL: 16245.149\n",
      "Step 600: Average NLL: 15066.479\n",
      "Step 800: Average NLL: 14537.041\n",
      "Step 1000: Average NLL: 14306.798\n",
      "Step 1200: Average NLL: 14185.054\n",
      "Step 1400: Average NLL: 14089.348\n",
      "Average test LL: 17596.970\n",
      "Bits per dimension: 8.264\n",
      "Total Flops 178528768\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + cp-t + quad-graph + 32\"\n",
      "Step 200: Average NLL: 16986.229\n",
      "Step 400: Average NLL: 16089.903\n",
      "Step 600: Average NLL: 14866.492\n",
      "Step 800: Average NLL: 14350.047\n",
      "Step 1000: Average NLL: 14119.497\n",
      "Step 1200: Average NLL: 14004.122\n",
      "Step 1400: Average NLL: 13904.324\n",
      "Average test LL: 17406.372\n",
      "Bits per dimension: 8.175\n",
      "Total Flops 714097152\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + cp-t + quad-graph + 64\"\n",
      "Step 200: Average NLL: 16927.490\n",
      "Step 400: Average NLL: 15920.434\n",
      "Step 600: Average NLL: 14673.518\n",
      "Step 800: Average NLL: 14189.962\n",
      "Step 1000: Average NLL: 13983.403\n",
      "Step 1200: Average NLL: 13870.068\n",
      "Step 1400: Average NLL: 13769.118\n",
      "Average test LL: 17279.214\n",
      "Bits per dimension: 8.115\n",
      "Total Flops 2856354304\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + cp-t + quad-graph + 128\"\n",
      "Step 200: Average NLL: 16875.459\n",
      "Step 400: Average NLL: 15772.691\n",
      "Step 600: Average NLL: 14521.912\n",
      "Step 800: Average NLL: 14064.842\n",
      "Step 1000: Average NLL: 13865.261\n",
      "Step 1200: Average NLL: 13753.715\n",
      "Step 1400: Average NLL: 13645.765\n",
      "Average test LL: 17181.719\n",
      "Bits per dimension: 8.069\n",
      "Total Flops 11425350144\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + cp-t + quad-tree-2 + 16\"\n",
      "Step 200: Average NLL: 17083.656\n",
      "Step 400: Average NLL: 16141.244\n",
      "Step 600: Average NLL: 15049.265\n",
      "Step 800: Average NLL: 14577.955\n",
      "Step 1000: Average NLL: 14369.776\n",
      "Step 1200: Average NLL: 14266.244\n",
      "Step 1400: Average NLL: 14183.237\n",
      "Average test LL: 17734.314\n",
      "Bits per dimension: 8.329\n",
      "Total Flops 66981888\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + cp-t + quad-tree-2 + 32\"\n",
      "Step 200: Average NLL: 17005.080\n",
      "Step 400: Average NLL: 15990.377\n",
      "Step 600: Average NLL: 14863.542\n",
      "Step 800: Average NLL: 14407.327\n",
      "Step 1000: Average NLL: 14199.412\n",
      "Step 1200: Average NLL: 14107.859\n",
      "Step 1400: Average NLL: 14010.831\n",
      "Average test LL: 17560.736\n",
      "Bits per dimension: 8.247\n",
      "Total Flops 267919360\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + cp-t + quad-tree-2 + 64\"\n",
      "Step 200: Average NLL: 16939.758\n",
      "Step 400: Average NLL: 15825.019\n",
      "Step 600: Average NLL: 14690.954\n",
      "Step 800: Average NLL: 14260.653\n",
      "Step 1000: Average NLL: 14088.733\n",
      "Step 1200: Average NLL: 13968.174\n",
      "Step 1400: Average NLL: 13884.903\n",
      "Average test LL: 17451.241\n",
      "Bits per dimension: 8.196\n",
      "Total Flops 1071661056\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + cp-t + quad-tree-2 + 128\"\n",
      "Step 200: Average NLL: 16881.934\n",
      "Step 400: Average NLL: 15671.023\n",
      "Step 600: Average NLL: 14555.312\n",
      "Step 800: Average NLL: 14142.910\n",
      "Step 1000: Average NLL: 13954.932\n",
      "Step 1200: Average NLL: 13866.914\n",
      "Step 1400: Average NLL: 13757.114\n",
      "Average test LL: 17357.339\n",
      "Bits per dimension: 8.151\n",
      "Total Flops 4286611456\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + cp + quad-graph + 16\"\n",
      "Step 200: Average NLL: 17000.992\n",
      "Step 400: Average NLL: 16420.527\n",
      "Step 600: Average NLL: 15242.003\n",
      "Step 800: Average NLL: 14606.714\n",
      "Step 1000: Average NLL: 14322.410\n",
      "Step 1200: Average NLL: 14188.082\n",
      "Step 1400: Average NLL: 14083.837\n",
      "Average test LL: 17553.672\n",
      "Bits per dimension: 8.244\n",
      "Total Flops 312746496\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + cp + quad-graph + 32\"\n",
      "Step 200: Average NLL: 16938.027\n",
      "Step 400: Average NLL: 16269.253\n",
      "Step 600: Average NLL: 14992.968\n",
      "Step 800: Average NLL: 14389.069\n",
      "Step 1000: Average NLL: 14114.390\n",
      "Step 1200: Average NLL: 13966.604\n",
      "Step 1400: Average NLL: 13860.608\n",
      "Average test LL: 17313.016\n",
      "Bits per dimension: 8.131\n",
      "Total Flops 1250968064\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + cp + quad-graph + 64\"\n",
      "Step 200: Average NLL: 16892.646\n",
      "Step 400: Average NLL: 16136.287\n",
      "Step 600: Average NLL: 14797.988\n",
      "Step 800: Average NLL: 14210.194\n",
      "Step 1000: Average NLL: 13963.331\n",
      "Step 1200: Average NLL: 13819.860\n",
      "Step 1400: Average NLL: 13715.875\n",
      "Average test LL: 17158.421\n",
      "Bits per dimension: 8.058\n",
      "Total Flops 5003837952\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + cp + quad-graph + 128\"\n",
      "Step 200: Average NLL: 16858.496\n",
      "Step 400: Average NLL: 16015.052\n",
      "Step 600: Average NLL: 14624.285\n",
      "Step 800: Average NLL: 14078.496\n",
      "Step 1000: Average NLL: 13844.525\n",
      "Step 1200: Average NLL: 13710.650\n",
      "Step 1400: Average NLL: 13609.390\n",
      "Average test LL: 17059.333\n",
      "Bits per dimension: 8.012\n",
      "Total Flops 20015284736\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + cp + quad-tree-2 + 16\"\n",
      "Step 200: Average NLL: 17003.645\n",
      "Step 400: Average NLL: 16476.658\n",
      "Step 600: Average NLL: 15322.326\n",
      "Step 800: Average NLL: 14663.091\n",
      "Step 1000: Average NLL: 14379.061\n",
      "Step 1200: Average NLL: 14246.198\n",
      "Step 1400: Average NLL: 14133.004\n",
      "Average test LL: 17624.699\n",
      "Bits per dimension: 8.277\n",
      "Total Flops 134090752\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + cp + quad-tree-2 + 32\"\n",
      "Step 200: Average NLL: 16939.230\n",
      "Step 400: Average NLL: 16330.605\n",
      "Step 600: Average NLL: 15059.694\n",
      "Step 800: Average NLL: 14437.154\n",
      "Step 1000: Average NLL: 14169.970\n",
      "Step 1200: Average NLL: 14026.924\n",
      "Step 1400: Average NLL: 13917.680\n",
      "Average test LL: 17379.930\n",
      "Bits per dimension: 8.162\n",
      "Total Flops 536354816\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + cp + quad-tree-2 + 64\"\n",
      "Step 200: Average NLL: 16892.727\n",
      "Step 400: Average NLL: 16191.583\n",
      "Step 600: Average NLL: 14849.028\n",
      "Step 800: Average NLL: 14264.045\n",
      "Step 1000: Average NLL: 14002.762\n",
      "Step 1200: Average NLL: 13869.071\n",
      "Step 1400: Average NLL: 13771.024\n",
      "Average test LL: 17217.665\n",
      "Bits per dimension: 8.086\n",
      "Total Flops 2145402880\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + cp + quad-tree-2 + 128\"\n",
      "Step 200: Average NLL: 16858.600\n",
      "Step 400: Average NLL: 16063.732\n",
      "Step 600: Average NLL: 14671.686\n",
      "Step 800: Average NLL: 14113.322\n",
      "Step 1000: Average NLL: 13876.011\n",
      "Step 1200: Average NLL: 13740.913\n",
      "Step 1400: Average NLL: 13634.301\n",
      "Average test LL: 17095.730\n",
      "Bits per dimension: 8.029\n",
      "Total Flops 8581578752\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + tucker + quad-graph + 16\"\n",
      "Step 200: Average NLL: 17001.883\n",
      "Step 400: Average NLL: 16510.424\n",
      "Step 600: Average NLL: 15413.445\n",
      "Step 800: Average NLL: 14694.406\n",
      "Step 1000: Average NLL: 14387.995\n",
      "Step 1200: Average NLL: 14208.607\n",
      "Step 1400: Average NLL: 14091.166\n",
      "Average test LL: 17563.239\n",
      "Bits per dimension: 8.248\n",
      "Total Flops 2187993600\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + tucker + quad-graph + 32\"\n",
      "Step 200: Average NLL: 16938.779\n",
      "Step 400: Average NLL: 16397.373\n",
      "Step 600: Average NLL: 15242.811\n",
      "Step 800: Average NLL: 14544.691\n",
      "Step 1000: Average NLL: 14239.600\n",
      "Step 1200: Average NLL: 14076.373\n",
      "Step 1400: Average NLL: 13940.416\n",
      "Average test LL: 17401.076\n",
      "Bits per dimension: 8.172\n",
      "Total Flops 17325113856\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + tucker + quad-graph + 64\"\n",
      "Step 200: Average NLL: 16893.184\n",
      "Step 400: Average NLL: 16310.168\n",
      "Step 600: Average NLL: 15126.746\n",
      "Step 800: Average NLL: 14429.884\n",
      "Step 1000: Average NLL: 14118.315\n",
      "Step 1200: Average NLL: 13974.198\n",
      "Step 1400: Average NLL: 13825.439\n",
      "Average test LL: 17301.093\n",
      "Bits per dimension: 8.125\n",
      "Total Flops 137885680128\n",
      "Total Memory cost 44687756800\n",
      "\n",
      "Training circuit \"base + tucker + quad-graph + 128\"\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.00 GiB. GPU 5 has a total capacity of 47.54 GiB of which 3.50 GiB is free. Process 3510311 has 492.00 MiB memory in use. Including non-PyTorch memory, this process has 43.55 GiB memory in use. Of the allocated memory 42.78 GiB is allocated by PyTorch, and 467.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 132\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining circuit \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m k)\n\u001b[1;32m    131\u001b[0m ctype\u001b[38;5;241m=\u001b[39mk\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m--> 132\u001b[0m results[k] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_eval_circuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctype\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m results[k][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    134\u001b[0m results[k][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum product layer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "Cell \u001b[0;32mIn[12], line 68\u001b[0m, in \u001b[0;36mtrain_and_eval_circuit\u001b[0;34m(cc, patch)\u001b[0m\n\u001b[1;32m     66\u001b[0m batch\u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Compute the log-likelihoods of the batch, by evaluating the circuit\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m log_likelihoods \u001b[38;5;241m=\u001b[39m \u001b[43mcircuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# We take the negated average log-likelihood as loss\u001b[39;00m\n\u001b[1;32m     71\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(log_likelihoods)\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/circuits.py:240\u001b[0m, in \u001b[0;36mTorchCircuit.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/circuits.py:261\u001b[0m, in \u001b[0;36mTorchCircuit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scope \u001b[38;5;129;01mand\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected some input \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, as the circuit has scope \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scope\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/circuits.py:273\u001b[0m, in \u001b[0;36mTorchCircuit._evaluate_layers\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_evaluate_layers\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# Evaluate layers on the given input\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (O, B, K)\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, O, K)\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;66;03m# If the circuit has empty scope, we squeeze the batch dimension, as it is 1\u001b[39;00m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/graph/modules.py:331\u001b[0m, in \u001b[0;36mTorchDiAcyclicGraph.evaluate\u001b[0;34m(self, x, module_fn)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     y \u001b[38;5;241m=\u001b[39m module_fn(module, \u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/layers/inner.py:50\u001b[0m, in \u001b[0;36mTorchInnerLayer.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/layers/optimized.py:88\u001b[0m, in \u001b[0;36mTorchTuckerLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# x: (F, H, B, Ki)\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# weight: (F, Ko, Ki ** arity) -> (F, Ko, Ki, ..., Ki)\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_output_units, \u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_input_units \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marity))\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msemiring\u001b[38;5;241m.\u001b[39meinsum(\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_einsum,\n\u001b[1;32m     93\u001b[0m         inputs\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39munbind(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m         keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     97\u001b[0m     )\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/parameters/parameter.py:178\u001b[0m, in \u001b[0;36mTorchParameter.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/parameters/parameter.py:188\u001b[0m, in \u001b[0;36mTorchParameter.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate the parameter computational graph.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m            of each parameter tensor slice.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/graph/modules.py:331\u001b[0m, in \u001b[0;36mTorchDiAcyclicGraph.evaluate\u001b[0;34m(self, x, module_fn)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     y \u001b[38;5;241m=\u001b[39m module_fn(module, \u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/parameters/nodes.py:306\u001b[0m, in \u001b[0;36mTorchUnaryParameterOp.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/parameters/nodes.py:675\u001b[0m, in \u001b[0;36mTorchSoftmaxParameter.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 5 has a total capacity of 47.54 GiB of which 3.50 GiB is free. Process 3510311 has 492.00 MiB memory in use. Including non-PyTorch memory, this process has 43.55 GiB memory in use. Of the allocated memory 42.78 GiB is allocated by PyTorch, and 467.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "from cirkit.pipeline import compile\n",
    "\n",
    "def get_flops(model, inp, with_backward=False):\n",
    "    \n",
    "    istrain = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    inp = inp if isinstance(inp, torch.Tensor) else torch.randn(inp)\n",
    "\n",
    "    flop_counter = FlopCounterMode(mods=model, display=False, depth=None)\n",
    "    with flop_counter:\n",
    "        if with_backward:\n",
    "            model(inp).sum().backward()\n",
    "        else:\n",
    "            model(inp)\n",
    "    total_flops =  flop_counter.get_total_flops()\n",
    "    if istrain:\n",
    "        model.train()\n",
    "    return total_flops\n",
    "\n",
    "def train_and_eval_circuit(cc,patch:bool):\n",
    "\n",
    "    torch.cuda.memory.reset_peak_memory_stats()\n",
    "    # Set some seeds\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    # torch.cuda.manual_seed(42)\n",
    "    \n",
    "    # Set the torch device to use\n",
    "    device = torch.device(DEVICE)\n",
    "    # Move the circuit to chosen device\n",
    "    circuit =compile(cc)\n",
    "    circuit = circuit.to(device)\n",
    "    \n",
    "    num_epochs = 5\n",
    "    step_idx = 0\n",
    "    running_loss = 0.0\n",
    "    running_samples = 0\n",
    "    stats = dict()\n",
    "\n",
    "    stats['# trainable parameters'] = sum(p.numel() for p in circuit.parameters() if p.requires_grad)\n",
    "    stats['train loss'] = []\n",
    "    patch_fn=patchify(KERNEL_SIZE, KERNEL_SIZE)\n",
    "    # Initialize a torch optimizer of your choice,\n",
    "    #  e.g., Adam, by passing the parameters of the circuit\n",
    "    optimizer = torch.optim.Adam(circuit.parameters(), lr=0.01)\n",
    "    begin_train=time.time()\n",
    "    keep_batch=None\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        for i, (batch, _) in enumerate(train_dataloader):\n",
    "            # The circuit expects an input of shape (batch_dim, num_variables)\n",
    "            if patch:\n",
    "                batch = patch_fn(batch)\n",
    "            BS = batch.shape[0]\n",
    "            batch = batch.view(BS, -1)\n",
    "            if keep_batch is  None:\n",
    "                keep_batch=batch\n",
    "            batch= batch.to(device)\n",
    "            # Compute the log-likelihoods of the batch, by evaluating the circuit\n",
    "            log_likelihoods = circuit(batch)\n",
    "    \n",
    "            # We take the negated average log-likelihood as loss\n",
    "            loss = -torch.mean(log_likelihoods)\n",
    "            loss.backward()\n",
    "            # Update the parameters of the circuits, as any other model in PyTorch\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            running_loss += loss.detach() * len(batch)\n",
    "            running_samples += len(batch)\n",
    "            step_idx += 1\n",
    "            if step_idx % 200 == 0:\n",
    "                average_nll = running_loss / running_samples\n",
    "                print(f\"Step {step_idx}: Average NLL: {average_nll:.3f}\")\n",
    "                running_loss = 0.0\n",
    "                running_samples = 0\n",
    "                \n",
    "                stats['train loss'].append(average_nll.cpu().item())\n",
    "    end_train=time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_lls = 0.0\n",
    "    \n",
    "        for batch, _ in val_dataloader:\n",
    "            # The circuit expects an input of shape (batch_dim, num_variables)\n",
    "            if patch:\n",
    "                batch = patch_fn(batch)\n",
    "            BS = batch.shape[0]\n",
    "            batch = batch.view(BS, -1).to(device)\n",
    "    \n",
    "            # Compute the log-likelihoods of the batch\n",
    "            log_likelihoods = circuit(batch)\n",
    "    \n",
    "            # Accumulate the log-likelihoods\n",
    "            test_lls += log_likelihoods.sum().item()\n",
    "    \n",
    "        # Compute average test log-likelihood and bits per dimension\n",
    "        average_nll = - test_lls / len(data_test)\n",
    "        bpd = average_nll / (32 * 32 * 3 * np.log(2.0))\n",
    "        print(f\"Average test LL: {average_nll:.3f}\")\n",
    "        print(f\"Bits per dimension: {bpd:.3f}\")\n",
    "        \n",
    "        stats['test loss'] = average_nll\n",
    "        stats['test bits per dimension'] = bpd\n",
    "    end_test=time.time()\n",
    "\n",
    "    stats['train loss (min)'] = min(stats['train loss'])\n",
    "    stats['train time']= end_train - begin_train\n",
    "    stats['test time']= end_test - end_train\n",
    "    stats[\"FLOPs\"]= get_flops(circuit, keep_batch.to(device))\n",
    "    stats[\"memory\"]= torch.cuda.memory.max_memory_allocated(device)\n",
    "    print(f\"Total Flops {stats['FLOPs']}\")\n",
    "    print(f\"Total Memory cost {stats['memory']}\")\n",
    "\n",
    "    # Free GPU memory\n",
    "    circuit = circuit.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return stats\n",
    "\n",
    "results = dict()\n",
    "for k, cc in circuits.items():\n",
    "    print('\\nTraining circuit \"%s\"' % k)\n",
    "    ctype=k.split('+')[0].strip()\n",
    "    results[k] = train_and_eval_circuit(cc, patch=ctype==\"patch\")\n",
    "    results[k]['type'] = k.split('+')[0].strip()\n",
    "    results[k]['sum product layer'] = k.split('+')[1].strip()\n",
    "    results[k]['structure'] = k.split('+')[2].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1046386d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finish Scanning model in </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.0177</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> seconds</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinish Scanning model in \u001b[0m\u001b[1;32m0.0177\u001b[0m\u001b[1;34m seconds\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-266.7532]],\n",
      "\n",
      "        [[-268.8200]],\n",
      "\n",
      "        [[-268.7206]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-267.1960]],\n",
      "\n",
      "        [[-268.8501]],\n",
      "\n",
      "        [[-268.8837]]], device='cuda:6', grad_fn=<TransposeBackward0>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m res \u001b[38;5;241m=\u001b[39m circuit(batch)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mcircuit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torchmeter/core.py:1429\u001b[0m, in \u001b[0;36mMeter.overview\u001b[0;34m(self, show_warning, *order)\u001b[0m\n\u001b[1;32m   1426\u001b[0m format_cell \u001b[38;5;241m=\u001b[39m partial(Panel, safe_box\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, expand\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, highlight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, box\u001b[38;5;241m=\u001b[39mHORIZONTALS)\n\u001b[1;32m   1428\u001b[0m container\u001b[38;5;241m.\u001b[39madd_renderable(format_cell(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_info, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[b]Model INFO[/]\u001b[39m\u001b[38;5;124m\"\u001b[39m, border_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morange1\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 1429\u001b[0m container\u001b[38;5;241m.\u001b[39mrenderables\u001b[38;5;241m.\u001b[39mextend([\n\u001b[1;32m   1430\u001b[0m     format_cell(\n\u001b[1;32m   1431\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstat_info(stat_name, show_warning\u001b[38;5;241m=\u001b[39mshow_warning),\n\u001b[1;32m   1432\u001b[0m         title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[b]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstat_name\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m INFO[/]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1433\u001b[0m         border_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcyan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1434\u001b[0m     )\n\u001b[1;32m   1435\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stat_name \u001b[38;5;129;01min\u001b[39;00m order\n\u001b[1;32m   1436\u001b[0m ])\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m container\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torchmeter/core.py:1431\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1426\u001b[0m format_cell \u001b[38;5;241m=\u001b[39m partial(Panel, safe_box\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, expand\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, highlight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, box\u001b[38;5;241m=\u001b[39mHORIZONTALS)\n\u001b[1;32m   1428\u001b[0m container\u001b[38;5;241m.\u001b[39madd_renderable(format_cell(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_info, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[b]Model INFO[/]\u001b[39m\u001b[38;5;124m\"\u001b[39m, border_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morange1\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1429\u001b[0m container\u001b[38;5;241m.\u001b[39mrenderables\u001b[38;5;241m.\u001b[39mextend([\n\u001b[1;32m   1430\u001b[0m     format_cell(\n\u001b[0;32m-> 1431\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstat_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_warning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_warning\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1432\u001b[0m         title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[b]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstat_name\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m INFO[/]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1433\u001b[0m         border_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcyan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1434\u001b[0m     )\n\u001b[1;32m   1435\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stat_name \u001b[38;5;129;01min\u001b[39;00m order\n\u001b[1;32m   1436\u001b[0m ])\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m container\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torchmeter/core.py:1313\u001b[0m, in \u001b[0;36mMeter.stat_info\u001b[0;34m(self, stat_or_statname, show_warning)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generates a formatted summary of the specified statistics.\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03mThis method provides a summary of the given statistics, including its name and the crucial data\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stat_or_statname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1313\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstat_or_statname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stat_or_statname, Statistics):\n\u001b[1;32m   1315\u001b[0m     stat \u001b[38;5;241m=\u001b[39m stat_or_statname\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torchmeter/core.py:993\u001b[0m, in \u001b[0;36mMeter.cal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# feed forwad\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ipt2device()\n\u001b[0;32m--> 993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mipt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mipt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;66;03m# remove hooks after measurement\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mremove() \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, hook_ls))\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/circuits.py:240\u001b[0m, in \u001b[0;36mTorchCircuit.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1881\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1880\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1883\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1885\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1886\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1829\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1826\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1827\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1829\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1832\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1833\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1834\u001b[0m     ):\n\u001b[1;32m   1835\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/circuits.py:261\u001b[0m, in \u001b[0;36mTorchCircuit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scope \u001b[38;5;129;01mand\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected some input \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, as the circuit has scope \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scope\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/circuits.py:273\u001b[0m, in \u001b[0;36mTorchCircuit._evaluate_layers\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_evaluate_layers\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# Evaluate layers on the given input\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (O, B, K)\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, O, K)\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;66;03m# If the circuit has empty scope, we squeeze the batch dimension, as it is 1\u001b[39;00m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/graph/modules.py:331\u001b[0m, in \u001b[0;36mTorchDiAcyclicGraph.evaluate\u001b[0;34m(self, x, module_fn)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     y \u001b[38;5;241m=\u001b[39m module_fn(module, \u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/layers/input.py:130\u001b[0m, in \u001b[0;36mTorchInputFunctionLayer.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1881\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1880\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1883\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1885\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1886\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1829\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1826\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1827\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1829\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1832\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1833\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1834\u001b[0m     ):\n\u001b[1;32m   1835\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/layers/input.py:277\u001b[0m, in \u001b[0;36mTorchExpFamilyLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 277\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_unnormalized_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msemiring\u001b[38;5;241m.\u001b[39mmap_from(x, LSESumSemiring)\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/layers/input.py:407\u001b[0m, in \u001b[0;36mTorchCategoricalLayer.log_unnormalized_likelihood\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m     logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    409\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits()\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/parameters/parameter.py:178\u001b[0m, in \u001b[0;36mTorchParameter.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1881\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1880\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1883\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1885\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1886\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1829\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1826\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1827\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1829\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1832\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1833\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1834\u001b[0m     ):\n\u001b[1;32m   1835\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/parameters/parameter.py:188\u001b[0m, in \u001b[0;36mTorchParameter.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate the parameter computational graph.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m            of each parameter tensor slice.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/graph/modules.py:331\u001b[0m, in \u001b[0;36mTorchDiAcyclicGraph.evaluate\u001b[0;34m(self, x, module_fn)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     y \u001b[38;5;241m=\u001b[39m module_fn(module, \u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/cirkit/backend/torch/parameters/nodes.py:61\u001b[0m, in \u001b[0;36mTorchParameterInput.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1881\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1880\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1883\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1885\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1886\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1842\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1840\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1841\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1842\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1845\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torchmeter/statistic.py:564\u001b[0m, in \u001b[0;36mCalMeter.__not_support_hook\u001b[0;34m(self, module, ipt, opt)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_not_supported \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__stat_ls):\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__stat_ls\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetail_val_container(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    561\u001b[0m             Operation_Id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opnode\u001b[38;5;241m.\u001b[39mnode_id,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    562\u001b[0m             Operation_Name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opnode\u001b[38;5;241m.\u001b[39mname,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    563\u001b[0m             Operation_Type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opnode\u001b[38;5;241m.\u001b[39mtype,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 564\u001b[0m             Input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__iopt_repr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mipt\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    565\u001b[0m             Output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__iopt_repr(opt),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    566\u001b[0m         )\n\u001b[1;32m    567\u001b[0m     )\n",
      "File \u001b[0;32m/disk/scratch/s2893001/cirkit/.conda/lib/python3.10/site-packages/torchmeter/statistic.py:352\u001b[0m, in \u001b[0;36mCalMeter.__iopt_repr\u001b[0;34m(self, iopt)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(iopt, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mset\u001b[39m)):\n\u001b[1;32m    351\u001b[0m     item_repr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__iopt_repr, iopt))\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(item_repr) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(item_repr) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mitem_repr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(iopt, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    355\u001b[0m     item_repr \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__iopt_repr(k), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__iopt_repr(v)) \n\u001b[1;32m    356\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m iopt\u001b[38;5;241m.\u001b[39mitems()]  \u001b[38;5;66;03m# fmt: skip\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# circuit =compile(list(circuits.values())[3])\n",
    "# circuit = Meter(circuit)\n",
    "# circuit.to(\"cuda:6\")\n",
    "batch, _ = next(iter(val_dataloader))\n",
    "patch_fn=patchify(KERNEL_SIZE, KERNEL_SIZE)\n",
    "\n",
    "batch = patch_fn(batch)\n",
    "BS = batch.shape[0]\n",
    "batch = batch.view(BS, -1).to(\"cuda:6\")\n",
    "\n",
    "res = circuit(batch)\n",
    "print(res)\n",
    "circuit.overview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d85ddeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># trainable parameters</th>\n",
       "      <th>test loss</th>\n",
       "      <th>test bits per dimension</th>\n",
       "      <th>train loss (min)</th>\n",
       "      <th>train time</th>\n",
       "      <th>test time</th>\n",
       "      <th>type</th>\n",
       "      <th>sum product layer</th>\n",
       "      <th>structure</th>\n",
       "      <th>train time format</th>\n",
       "      <th>test time format</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>patch</th>\n",
       "      <td>8,135,170</td>\n",
       "      <td>12,483.816</td>\n",
       "      <td>5.863</td>\n",
       "      <td>194.937</td>\n",
       "      <td>1,048.771</td>\n",
       "      <td>3.044</td>\n",
       "      <td>patch</td>\n",
       "      <td>tucker</td>\n",
       "      <td>quad-graph</td>\n",
       "      <td>01:28</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patch</th>\n",
       "      <td>4,460,544</td>\n",
       "      <td>12,647.629</td>\n",
       "      <td>5.940</td>\n",
       "      <td>197.561</td>\n",
       "      <td>535.622</td>\n",
       "      <td>2.226</td>\n",
       "      <td>patch</td>\n",
       "      <td>tucker</td>\n",
       "      <td>quad-tree-2</td>\n",
       "      <td>01:55</td>\n",
       "      <td>01:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patch</th>\n",
       "      <td>1,032,834</td>\n",
       "      <td>12,685.412</td>\n",
       "      <td>5.957</td>\n",
       "      <td>198.096</td>\n",
       "      <td>294.293</td>\n",
       "      <td>1.839</td>\n",
       "      <td>patch</td>\n",
       "      <td>cp</td>\n",
       "      <td>quad-graph</td>\n",
       "      <td>01:54</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patch</th>\n",
       "      <td>901,762</td>\n",
       "      <td>12,689.151</td>\n",
       "      <td>5.959</td>\n",
       "      <td>198.086</td>\n",
       "      <td>286.904</td>\n",
       "      <td>1.766</td>\n",
       "      <td>patch</td>\n",
       "      <td>cp.T</td>\n",
       "      <td>quad-graph</td>\n",
       "      <td>01:46</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patch</th>\n",
       "      <td>909,376</td>\n",
       "      <td>12,832.732</td>\n",
       "      <td>6.027</td>\n",
       "      <td>200.429</td>\n",
       "      <td>268.187</td>\n",
       "      <td>1.509</td>\n",
       "      <td>patch</td>\n",
       "      <td>cp</td>\n",
       "      <td>quad-tree-2</td>\n",
       "      <td>01:28</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patch</th>\n",
       "      <td>843,840</td>\n",
       "      <td>12,860.927</td>\n",
       "      <td>6.040</td>\n",
       "      <td>200.870</td>\n",
       "      <td>261.979</td>\n",
       "      <td>1.584</td>\n",
       "      <td>patch</td>\n",
       "      <td>cp.T</td>\n",
       "      <td>quad-tree-2</td>\n",
       "      <td>01:21</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>67,136,130</td>\n",
       "      <td>13,412.097</td>\n",
       "      <td>6.299</td>\n",
       "      <td>13,043.954</td>\n",
       "      <td>364.976</td>\n",
       "      <td>1.746</td>\n",
       "      <td>base</td>\n",
       "      <td>cp</td>\n",
       "      <td>quad-graph</td>\n",
       "      <td>01:04</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>586,205,698</td>\n",
       "      <td>13,430.028</td>\n",
       "      <td>6.307</td>\n",
       "      <td>13,018.108</td>\n",
       "      <td>1,864.905</td>\n",
       "      <td>3.985</td>\n",
       "      <td>base</td>\n",
       "      <td>tucker</td>\n",
       "      <td>quad-graph</td>\n",
       "      <td>01:04</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>58,712,128</td>\n",
       "      <td>13,458.212</td>\n",
       "      <td>6.320</td>\n",
       "      <td>13,103.154</td>\n",
       "      <td>300.240</td>\n",
       "      <td>1.567</td>\n",
       "      <td>base</td>\n",
       "      <td>cp</td>\n",
       "      <td>quad-tree-2</td>\n",
       "      <td>01:00</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>318,246,912</td>\n",
       "      <td>13,459.455</td>\n",
       "      <td>6.321</td>\n",
       "      <td>13,045.494</td>\n",
       "      <td>992.584</td>\n",
       "      <td>2.709</td>\n",
       "      <td>base</td>\n",
       "      <td>tucker</td>\n",
       "      <td>quad-tree-2</td>\n",
       "      <td>01:32</td>\n",
       "      <td>01:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>58,747,522</td>\n",
       "      <td>13,515.246</td>\n",
       "      <td>6.347</td>\n",
       "      <td>13,064.032</td>\n",
       "      <td>336.427</td>\n",
       "      <td>1.624</td>\n",
       "      <td>base</td>\n",
       "      <td>cp.T</td>\n",
       "      <td>quad-graph</td>\n",
       "      <td>01:36</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>54,517,824</td>\n",
       "      <td>13,755.585</td>\n",
       "      <td>6.460</td>\n",
       "      <td>13,258.001</td>\n",
       "      <td>273.429</td>\n",
       "      <td>1.506</td>\n",
       "      <td>base</td>\n",
       "      <td>cp.T</td>\n",
       "      <td>quad-tree-2</td>\n",
       "      <td>01:33</td>\n",
       "      <td>01:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       # trainable parameters  test loss  test bits per dimension  \\\n",
       "patch               8,135,170 12,483.816                    5.863   \n",
       "patch               4,460,544 12,647.629                    5.940   \n",
       "patch               1,032,834 12,685.412                    5.957   \n",
       "patch                 901,762 12,689.151                    5.959   \n",
       "patch                 909,376 12,832.732                    6.027   \n",
       "patch                 843,840 12,860.927                    6.040   \n",
       "base               67,136,130 13,412.097                    6.299   \n",
       "base              586,205,698 13,430.028                    6.307   \n",
       "base               58,712,128 13,458.212                    6.320   \n",
       "base              318,246,912 13,459.455                    6.321   \n",
       "base               58,747,522 13,515.246                    6.347   \n",
       "base               54,517,824 13,755.585                    6.460   \n",
       "\n",
       "        train loss (min)  train time  test time   type sum product layer  \\\n",
       "patch            194.937   1,048.771      3.044  patch            tucker   \n",
       "patch            197.561     535.622      2.226  patch            tucker   \n",
       "patch            198.096     294.293      1.839  patch                cp   \n",
       "patch            198.086     286.904      1.766  patch              cp.T   \n",
       "patch            200.429     268.187      1.509  patch                cp   \n",
       "patch            200.870     261.979      1.584  patch              cp.T   \n",
       "base          13,043.954     364.976      1.746   base                cp   \n",
       "base          13,018.108   1,864.905      3.985   base            tucker   \n",
       "base          13,103.154     300.240      1.567   base                cp   \n",
       "base          13,045.494     992.584      2.709   base            tucker   \n",
       "base          13,064.032     336.427      1.624   base              cp.T   \n",
       "base          13,258.001     273.429      1.506   base              cp.T   \n",
       "\n",
       "          structure train time format test time format  \n",
       "patch    quad-graph             01:28            01:03  \n",
       "patch   quad-tree-2             01:55            01:02  \n",
       "patch    quad-graph             01:54            01:01  \n",
       "patch    quad-graph             01:46            01:01  \n",
       "patch   quad-tree-2             01:28            01:01  \n",
       "patch   quad-tree-2             01:21            01:01  \n",
       "base     quad-graph             01:04            01:01  \n",
       "base     quad-graph             01:04            01:03  \n",
       "base    quad-tree-2             01:00            01:01  \n",
       "base    quad-tree-2             01:32            01:02  \n",
       "base     quad-graph             01:36            01:01  \n",
       "base    quad-tree-2             01:33            01:01  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(results, orient='index')\n",
    "df = df.drop(columns='train loss')\n",
    "\n",
    "df.index = df.index.map(lambda x: x.split('+')[0])\n",
    "df[\"# trainable parameters\"] = df[\"# trainable parameters\"].map('{:,d}'.format)\n",
    "pd.options.display.float_format = \"{:,.3f}\".format\n",
    "df[\"train time format\"]=pd.to_datetime(df['train time'], unit='s').dt.strftime(\"%m:%S\")\n",
    "df[\"test time format\"]=pd.to_datetime(df['test time'], unit='s').dt.strftime(\"%m:%S\")\n",
    "\n",
    "df.sort_values('test bits per dimension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d635c345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllrrll}\n",
      "\\toprule\n",
      "type & sum product layer & structure & \\# trainable parameters & test bits per dimension & test loss & train time format & test time format \\\\\n",
      "\\midrule\n",
      "patch & tucker & quad-graph & 8,135,170 & 5.86 & 12483.82 & 01:28 & 01:03 \\\\\n",
      "patch & tucker & quad-tree-2 & 4,460,544 & 5.94 & 12647.63 & 01:55 & 01:02 \\\\\n",
      "patch & cp & quad-graph & 1,032,834 & 5.96 & 12685.41 & 01:54 & 01:01 \\\\\n",
      "patch & cp.T & quad-graph & 901,762 & 5.96 & 12689.15 & 01:46 & 01:01 \\\\\n",
      "patch & cp & quad-tree-2 & 909,376 & 6.03 & 12832.73 & 01:28 & 01:01 \\\\\n",
      "patch & cp.T & quad-tree-2 & 843,840 & 6.04 & 12860.93 & 01:21 & 01:01 \\\\\n",
      "base & cp & quad-graph & 67,136,130 & 6.30 & 13412.10 & 01:04 & 01:01 \\\\\n",
      "base & tucker & quad-graph & 586,205,698 & 6.31 & 13430.03 & 01:04 & 01:03 \\\\\n",
      "base & cp & quad-tree-2 & 58,712,128 & 6.32 & 13458.21 & 01:00 & 01:01 \\\\\n",
      "base & tucker & quad-tree-2 & 318,246,912 & 6.32 & 13459.45 & 01:32 & 01:02 \\\\\n",
      "base & cp.T & quad-graph & 58,747,522 & 6.35 & 13515.25 & 01:36 & 01:01 \\\\\n",
      "base & cp.T & quad-tree-2 & 54,517,824 & 6.46 & 13755.58 & 01:33 & 01:01 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df[[\"type\", \"sum product layer\", \"structure\",\"# trainable parameters\", \"test bits per dimension\", \"test loss\", \"train time format\", \"test time format\"]].sort_values('test bits per dimension').to_latex(\n",
    "    float_format=\"%.2f\",\n",
    "    escape=True,\n",
    "    index=False\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b246223",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"bench_cifar.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
